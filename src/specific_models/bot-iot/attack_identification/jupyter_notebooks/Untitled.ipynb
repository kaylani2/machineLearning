{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/newuser/machineLearning/src/specific_models/bot-iot/attack_identification/jupyter_notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE: 0\n",
      "Reading ../../../../../datasets/bot-iot/UNSW_2018_IoT_Botnet_Full5pc_1.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-41eb4ea877c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;31m## Load dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;31m###############################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mFILE_NAME\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFIVE_PERCENT_FILES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mINDEX_COLUMN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNAN_VALUES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/machineLearning/src/specific_models/bot-iot/attack_identification/unit.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(file_schema, file_range, index_column, nan_values, verbose)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'Reading'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_schema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfile_number\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     aux = pd.read_csv (file_schema.format (str (file_number)),\n\u001b[0m\u001b[1;32m     20\u001b[0m                        \u001b[0mindex_col\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex_column\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m                        \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mindex_column\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/machineLearning-KVdAfBLK/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/machineLearning-KVdAfBLK/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/machineLearning-KVdAfBLK/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/machineLearning-KVdAfBLK/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/machineLearning-KVdAfBLK/lib/python3.8/site-packages/pandas/core/dtypes/common.py\u001b[0m in \u001b[0;36mis_categorical_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr_or_dtype\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m     \"\"\"\n\u001b[1;32m    544\u001b[0m     \u001b[0mCheck\u001b[0m \u001b[0mwhether\u001b[0m \u001b[0man\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlike\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mCategorical\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Author: Kaylani Bochie\n",
    "# github.com/kaylani2\n",
    "# kaylani AT gta DOT ufrj DOT br\n",
    "\n",
    "### K: Model: LSTM\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "sys.path.insert(1, '../')\n",
    "import numpy as np\n",
    "from numpy import mean, std\n",
    "from unit import remove_columns_with_one_value, remove_nan_columns, load_dataset\n",
    "from unit import display_general_information, display_feature_distribution\n",
    "from collections import Counter\n",
    "#from imblearn.over_sampling import RandomOverSampler, RandomUnderSampler\n",
    "import sklearn\n",
    "from sklearn import set_config\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif, chi2, mutual_info_classif\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.utils\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.constraints import maxnorm\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Define constants\n",
    "###############################################################################\n",
    "pd.set_option ('display.max_rows', None)\n",
    "pd.set_option ('display.max_columns', 5)\n",
    "BOT_IOT_DIRECTORY = '../../../../../datasets/bot-iot/'\n",
    "BOT_IOT_FEATURE_NAMES = 'UNSW_2018_IoT_Botnet_Dataset_Feature_Names.csv'\n",
    "BOT_IOT_FILE_5_PERCENT_SCHEMA = 'UNSW_2018_IoT_Botnet_Full5pc_{}.csv' # 1 - 4\n",
    "FIVE_PERCENT_FILES = 4\n",
    "BOT_IOT_FILE_FULL_SCHEMA = 'UNSW_2018_IoT_Botnet_Dataset_{}.csv' # 1 - 74\n",
    "FULL_FILES = 74\n",
    "FILE_NAME = BOT_IOT_DIRECTORY + BOT_IOT_FILE_5_PERCENT_SCHEMA\n",
    "FEATURES = BOT_IOT_DIRECTORY + BOT_IOT_FEATURE_NAMES\n",
    "NAN_VALUES = ['?', '.']\n",
    "TARGET = 'attack'\n",
    "INDEX_COLUMN = 'pkSeqID'\n",
    "LABELS = ['attack', 'category', 'subcategory']\n",
    "STATE = 0\n",
    "try:\n",
    "  STATE = int (sys.argv [1])\n",
    "except:\n",
    "  pass\n",
    "#for STATE in [1, 2, 3, 4, 5]:\n",
    "np.random.seed (STATE)\n",
    "print ('STATE:', STATE)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Load dataset\n",
    "###############################################################################\n",
    "df = load_dataset (FILE_NAME, FIVE_PERCENT_FILES, INDEX_COLUMN, NAN_VALUES)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Clean dataset\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "### Remove columns with only one value\n",
    "df, log = remove_columns_with_one_value (df, verbose = False)\n",
    "print (log)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "### Remove redundant columns, useless columns and unused targets\n",
    "### K: _number columns are numerical representations of other existing columns.\n",
    "### K: category and subcategory are other labels.\n",
    "### K: saddr and daddr may specialize the model to a single network\n",
    "redundant_columns = ['state_number', 'proto_number', 'flgs_number']\n",
    "other_targets = ['category', 'subcategory']\n",
    "misc_columns = ['saddr', 'daddr']\n",
    "print ('Removing redundant columns:', redundant_columns)\n",
    "print ('Removing useless targets:', other_targets)\n",
    "print ('Removing misc columns:', misc_columns)\n",
    "columns_to_remove = redundant_columns + other_targets + misc_columns\n",
    "df.drop (axis = 'columns', columns = columns_to_remove, inplace = True)\n",
    "\n",
    "###############################################################################\n",
    "### Remove NaN columns (with a lot of NaN values)\n",
    "df, log = remove_nan_columns (df, 1/2, verbose = False)\n",
    "print (log)\n",
    "\n",
    "###############################################################################\n",
    "### Encode categorical features\n",
    "print ('Encoding categorical features (ordinal encoding).')\n",
    "my_encoder = OrdinalEncoder ()\n",
    "df ['flgs'] = my_encoder.fit_transform (df ['flgs'].values.reshape (-1, 1))\n",
    "df ['proto'] = my_encoder.fit_transform (df ['proto'].values.reshape (-1, 1))\n",
    "df ['sport'] = my_encoder.fit_transform (df ['sport'].astype (str).values.reshape (-1, 1))\n",
    "df ['dport'] = my_encoder.fit_transform (df ['dport'].astype (str).values.reshape (-1, 1))\n",
    "df ['state'] = my_encoder.fit_transform (df ['state'].values.reshape (-1, 1))\n",
    "print ('Objects:', list (df.select_dtypes ( ['object']).columns))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Quick sanity check\n",
    "###############################################################################\n",
    "display_general_information (df)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Split dataset into train and test sets\n",
    "###############################################################################\n",
    "### K: Dataset is too big? Drop.\n",
    "drop_indices = np.random.choice (df.index, int (df.shape [0] * 0.9),\n",
    "                                  replace = False)\n",
    "df = df.drop (drop_indices)\n",
    "TEST_SIZE = 3/10\n",
    "VALIDATION_SIZE = 1/4\n",
    "print ('Splitting dataset (test/train):', TEST_SIZE)\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split (\n",
    "                                               df.loc [:, df.columns != TARGET],\n",
    "                                               df [TARGET],\n",
    "                                               test_size = TEST_SIZE,\n",
    "                                               random_state = STATE,)\n",
    "print ('Splitting dataset (validation/train):', VALIDATION_SIZE)\n",
    "X_train_df, X_val_df, y_train_df, y_val_df = train_test_split (\n",
    "                                             X_train_df,\n",
    "                                             y_train_df,\n",
    "                                             test_size = VALIDATION_SIZE,\n",
    "                                             random_state = STATE,)\n",
    "print ('X_train_df shape:', X_train_df.shape)\n",
    "print ('y_train_df shape:', y_train_df.shape)\n",
    "print ('X_val_df shape:', X_val_df.shape)\n",
    "print ('y_val_df shape:', y_val_df.shape)\n",
    "print ('X_test_df shape:', X_test_df.shape)\n",
    "print ('y_test_df shape:', y_test_df.shape)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Convert dataframe to a numpy array\n",
    "###############################################################################\n",
    "print ('\\nConverting dataframe to numpy array.')\n",
    "X_train = X_train_df.values\n",
    "y_train = y_train_df.values\n",
    "X_val = X_val_df.values\n",
    "y_val = y_val_df.values\n",
    "X_test = X_test_df.values\n",
    "y_test = y_test_df.values\n",
    "print ('X_train shape:', X_train.shape)\n",
    "print ('y_train shape:', y_train.shape)\n",
    "print ('X_val shape:', X_val.shape)\n",
    "print ('y_val shape:', y_val.shape)\n",
    "print ('X_test shape:', X_test.shape)\n",
    "print ('y_test shape:', y_test.shape)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Apply normalization\n",
    "###############################################################################\n",
    "### K: NOTE: Only use derived information from the train set to avoid leakage.\n",
    "print ('\\nApplying normalization.')\n",
    "startTime = time.time ()\n",
    "scaler = StandardScaler ()\n",
    "#scaler = MinMaxScaler (feature_range = (0, 1))\n",
    "scaler.fit (X_train)\n",
    "X_train = scaler.transform (X_train)\n",
    "X_val = scaler.transform (X_val)\n",
    "X_test = scaler.transform (X_test)\n",
    "print (str (time.time () - startTime), 'to normalize data.')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Perform feature selection\n",
    "###############################################################################\n",
    "NUMBER_OF_FEATURES = 9 #'all'\n",
    "print ('\\nSelecting top', NUMBER_OF_FEATURES, 'features.')\n",
    "startTime = time.time ()\n",
    "#fs = SelectKBest (score_func = mutual_info_classif, k = NUMBER_OF_FEATURES)\n",
    "### K: ~30 minutes to FAIL fit mutual_info_classif to 5% bot-iot\n",
    "#fs = SelectKBest (score_func = chi2, k = NUMBER_OF_FEATURES) # X must be >= 0\n",
    "### K: ~4 seconds to fit chi2 to 5% bot-iot (MinMaxScaler (0, 1))\n",
    "fs = SelectKBest (score_func = f_classif, k = NUMBER_OF_FEATURES)\n",
    "### K: ~4 seconds to fit f_classif to 5% bot-iot\n",
    "fs.fit (X_train, y_train)\n",
    "X_train = fs.transform (X_train)\n",
    "X_val = fs.transform (X_val)\n",
    "X_test = fs.transform (X_test)\n",
    "print (str (time.time () - startTime), 'to select features.')\n",
    "print ('X_train shape:', X_train.shape)\n",
    "print ('y_train shape:', y_train.shape)\n",
    "print ('X_val shape:', X_val.shape)\n",
    "print ('y_val shape:', y_val.shape)\n",
    "print ('X_test shape:', X_test.shape)\n",
    "print ('y_test shape:', y_test.shape)\n",
    "bestFeatures = []\n",
    "for feature in range (len (fs.scores_)):\n",
    "  bestFeatures.append ({'f': feature, 's': fs.scores_ [feature]})\n",
    "bestFeatures = sorted (bestFeatures, key = lambda k: k ['s'])\n",
    "for feature in bestFeatures:\n",
    "  print ('Feature %d: %f' % (feature ['f'], feature ['s']))\n",
    "\n",
    "#pyplot.bar ( [i for i in range (len (fs.scores_))], fs.scores_)\n",
    "#pyplot.show ()\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Rearrange samples for RNN\n",
    "###############################################################################\n",
    "print ('\\nRearranging dataset for the RNN.')\n",
    "print ('X_train shape:', X_train.shape)\n",
    "print ('y_train shape:', y_train.shape)\n",
    "print ('X_val shape:', X_val.shape)\n",
    "print ('y_val shape:', y_val.shape)\n",
    "print ('X_test shape:', X_test.shape)\n",
    "print ('y_test shape:', y_test.shape)\n",
    "\n",
    "\n",
    "### K: JUMPING WINDOWS APPROACH: WRONG!!!\n",
    "#if ( (X_train.shape [0] % STEPS) != 0):\n",
    "#  X_train = X_train [:- (X_train.shape [0] % STEPS), :]\n",
    "#\n",
    "#X_train = X_train.reshape ( (X_train.shape [0] // STEPS, STEPS,\n",
    "#                            X_train.shape [1]),\n",
    "#                            order = 'C')\n",
    "#startTime = time.time ()\n",
    "#\n",
    "## X_train\n",
    "#if ( (X_train.shape [0] % STEPS) != 0):\n",
    "#  X_train = X_train [:- (X_train.shape [0] % STEPS), :]\n",
    "#X_train = X_train.reshape ( (X_train.shape [0] // STEPS, STEPS, X_train.shape [1]),\n",
    "#                           order = 'C')\n",
    "#print ('Finished X_train.')\n",
    "#\n",
    "## X_val\n",
    "#if ( (X_val.shape [0] % STEPS) != 0):\n",
    "#  X_val = X_val [:- (X_val.shape [0] % STEPS), :]\n",
    "#X_val = X_val.reshape ( (X_val.shape [0] // STEPS, STEPS, X_val.shape [1]),\n",
    "#                       order = 'C')\n",
    "#print ('Finished X_val.')\n",
    "#\n",
    "## X_test\n",
    "#if ( (X_test.shape [0] % STEPS) != 0):\n",
    "#  X_test = X_test [:- (X_test.shape [0] % STEPS), :]\n",
    "#X_test = X_test.reshape ( (X_test.shape [0] // STEPS, STEPS, X_test.shape [1]),\n",
    "#                          order = 'C')\n",
    "#print ('Finished X_test.')\n",
    "#\n",
    "## Y_train\n",
    "#if ( (y_train.shape [0] % STEPS) != 0):\n",
    "#  y_train = y_train [:- (y_train.shape [0] % STEPS)]\n",
    "#y_train = y_train.reshape ( (y_train.shape [0] // STEPS, STEPS), order = 'C')\n",
    "#\n",
    "## Y_val\n",
    "#if ( (y_val.shape [0] % STEPS) != 0):\n",
    "#  y_val = y_val [:- (y_val.shape [0] % STEPS)]\n",
    "#y_val = y_val.reshape ( (y_val.shape [0] // STEPS, STEPS), order = 'C')\n",
    "#\n",
    "## Y_test\n",
    "#if ( (y_test.shape [0] % STEPS) != 0):\n",
    "#  y_test = y_test [:- (y_test.shape [0] % STEPS)]\n",
    "#y_test = y_test.reshape ( (y_test.shape [0] // STEPS, STEPS), order = 'C')\n",
    "#\n",
    "#print (str (time.time () - startTime), 's reshape data.')\n",
    "\n",
    "\n",
    "### SLIDING WINDOW APPROACH: TAKES TOO LONG!\n",
    "#from numpy import array\n",
    "#LENGTH = 5\n",
    "#\n",
    "#sets_list = [X_train, X_test]\n",
    "#for index, data in enumerate (sets_list):\n",
    "#    n = data.shape [0]\n",
    "#    samples = []\n",
    "#\n",
    "#    # step over the X_train.shape [0] (samples) in jumps of 200 (time_steps)\n",
    "#    for i in range (0,n,LENGTH):\n",
    "#        print ('index, i1:', index, i)\n",
    "#        # grab from i to i + 200\n",
    "#        sample = data [i:i+LENGTH]\n",
    "#        samples.append (sample)\n",
    "#\n",
    "#    # convert list of arrays into 2d array\n",
    "#    new_data = list ()\n",
    "#    new_data = np.array (new_data)\n",
    "#    for i in range (len (samples)):\n",
    "#        print ('index, i2:', index, i)\n",
    "#        new_data = np.append (new_data, samples [i])\n",
    "#\n",
    "#    sets_list [index] = new_data.reshape (len (samples), LENGTH, data.shape [1])\n",
    "#\n",
    "#\n",
    "#X_train = sets_list [0]\n",
    "#X_test = sets_list [1]\n",
    "\n",
    "### SLIDING WINDOW: JUST RIGHT!\n",
    "\n",
    "STEPS = 3\n",
    "FEATURES = X_train.shape [1]\n",
    "def window_stack (a, stride = 1, numberOfSteps = 3):\n",
    "    return np.hstack ( [ a [i:1+i-numberOfSteps or None:stride] for i in range (0,numberOfSteps) ])\n",
    "\n",
    "X_train = window_stack (X_train, stride = 1, numberOfSteps = STEPS)\n",
    "X_train = X_train.reshape (X_train.shape [0], STEPS, FEATURES)\n",
    "X_val = window_stack (X_val, stride = 1, numberOfSteps = STEPS)\n",
    "X_val = X_val.reshape (X_val.shape [0], STEPS, FEATURES)\n",
    "X_test = window_stack (X_test, stride = 1, numberOfSteps = STEPS)\n",
    "X_test = X_test.reshape (X_test.shape [0], STEPS, FEATURES)\n",
    "\n",
    "y_train = y_train [ (STEPS - 1):]\n",
    "y_val = y_val [ (STEPS - 1):]\n",
    "y_test = y_test [ (STEPS - 1):]\n",
    "\n",
    "print ('X_train shape:', X_train.shape)\n",
    "print ('y_train shape:', y_train.shape)\n",
    "print ('X_val shape:', X_val.shape)\n",
    "print ('y_val shape:', y_val.shape)\n",
    "print ('X_test shape:', X_test.shape)\n",
    "print ('y_test shape:', y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Create learning model (Multilayer Perceptron) and tune hyperparameters\n",
    "###############################################################################\n",
    "### K: One hot encode the output.\n",
    "#numberOfClasses = len (df [TARGET].unique ())\n",
    "#print ('y_val:')\n",
    "#print (y_val [:50])\n",
    "#print (y_val.shape)\n",
    "#y_train = keras.utils.to_categorical (y_train, numberOfClasses)\n",
    "#y_val = keras.utils.to_categorical (y_val, numberOfClasses)\n",
    "#y_test = keras.utils.to_categorical (y_test, numberOfClasses)\n",
    "\n",
    "### -1 indices -> train\n",
    "### 0  indices -> validation\n",
    "test_fold = np.repeat ( [-1, 0], [X_train.shape [0], X_val.shape [0]])\n",
    "myPreSplit = PredefinedSplit (test_fold)\n",
    "\n",
    "\n",
    "print ('y_val:')\n",
    "print (y_val [:50])\n",
    "print (y_val.shape)\n",
    "#y_val = y_val.argmax (axis = 1)\n",
    "print ('y_val:')\n",
    "print (y_val [:50])\n",
    "print (y_val.shape)\n",
    "#y_train = y_train.argmax (axis = 1)\n",
    "\n",
    "#def create_model (learn_rate = 0.01, dropout_rate = 0.0, weight_constraint = 0):\n",
    "#  model = Sequential ()\n",
    "#  model.add (LSTM (50, activation= 'relu' , input_shape= (X_train.shape [1], X_train.shape [2])))\n",
    "#  model.add (Dense (1, activation = 'sigmoid'))\n",
    "#  model.compile (optimizer = 'adam', loss = 'binary_crossentropy',)\n",
    "#  return model\n",
    "#\n",
    "#model = KerasClassifier (build_fn = create_model, verbose = 2)\n",
    "#batch_size = [30]#10, 30, 50]\n",
    "#epochs = [3]#, 5, 10]\n",
    "#learn_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "#dropout_rate = [0.0]#, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "#weight_constraint = [0]#, 2, 3, 4, 5]\n",
    "#param_grid = dict (batch_size = batch_size, epochs = epochs,\n",
    "#                   dropout_rate = dropout_rate, learn_rate = learn_rate,\n",
    "#                   weight_constraint = weight_constraint)\n",
    "#grid = GridSearchCV (estimator = model, param_grid = param_grid,\n",
    "#                     scoring = 'f1_weighted', cv = myPreSplit, verbose = 2,\n",
    "#                     n_jobs = -1)\n",
    "#\n",
    "#grid_result = grid.fit (np.concatenate ( (X_train, X_val), axis = 0),\n",
    "#                        np.concatenate ( (y_train, y_val), axis = 0))\n",
    "#print (grid_result.best_params_)\n",
    "#\n",
    "#print (\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "#means = grid_result.cv_results_ ['mean_test_score']\n",
    "#stds = grid_result.cv_results_ ['std_test_score']\n",
    "#params = grid_result.cv_results_ ['params']\n",
    "#for mean, stdev, param in zip (means, stds, params):\n",
    "#  print (\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "#sys.exit ()\n",
    "\n",
    "\n",
    "#Best: 0.999989 using {'epochs': 3, 'learn_rate': 0.001, 'weight_constraint': 0, 'batch_size': 30, 'dropout_rate': 0.0}\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Finished model\n",
    "METRICS = [keras.metrics.TruePositives (name = 'TP'),\n",
    "           keras.metrics.FalsePositives (name = 'FP'),\n",
    "           keras.metrics.TrueNegatives (name = 'TN'),\n",
    "           keras.metrics.FalseNegatives (name = 'FN'),\n",
    "           keras.metrics.BinaryAccuracy (name = 'Acc.'),\n",
    "           keras.metrics.Precision (name = 'Prec.'),\n",
    "           keras.metrics.Recall (name = 'Recall'),\n",
    "           keras.metrics.AUC (name = 'AUC'),]\n",
    "BATCH_SIZE = 30\n",
    "NUMBER_OF_EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "WEIGHT_CONSTRAINT = 1\n",
    "clf = Sequential ()\n",
    "clf.add (LSTM (50, activation = 'relu',\n",
    "                     input_shape = (X_train.shape [1], X_train.shape [2])))\n",
    "clf.add (Dense (1, activation = 'sigmoid'))\n",
    "\n",
    "print ('Model summary:')\n",
    "clf.summary ()\n",
    "\n",
    "###############################################################################\n",
    "## Compile the network\n",
    "###############################################################################\n",
    "print ('\\nCompiling the network.')\n",
    "clf.compile (optimizer = 'adam',\n",
    "                   loss = 'binary_crossentropy',\n",
    "                   metrics = METRICS)\n",
    "\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Fit the network\n",
    "###############################################################################\n",
    "print ('\\nFitting the network.')\n",
    "startTime = time.time ()\n",
    "history = clf.fit (X_train, y_train,\n",
    "                         batch_size = BATCH_SIZE,\n",
    "                         epochs = NUMBER_OF_EPOCHS,\n",
    "                         verbose = 2, #1 = progress bar, not useful for logging\n",
    "                         workers = 0,\n",
    "                         use_multiprocessing = True,\n",
    "                         #class_weight = 'auto',\n",
    "                         validation_data = (X_val, y_val))\n",
    "#clf.fit (X_train, y_train, epochs = NUMBER_OF_EPOCHS,\n",
    "               #use_multiprocessing = True, verbose = 2)\n",
    "print (str (time.time () - startTime), 's to train model.')\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Analyze results\n",
    "###############################################################################\n",
    "#print ('y_val:')\n",
    "#print (y_val [:50])\n",
    "#print (y_val.shape)\n",
    "#y_val = y_val.reshape (y_val.shape [0], 1))\n",
    "#print ('y_val after reshape:')\n",
    "#print (y_val.shape)\n",
    "#y_val = y_val.argmax (axis = 1)\n",
    "#print ('y_pred:')\n",
    "#print (y_pred [:50])\n",
    "#print (y_pred.shape)\n",
    "#print ('y_pred after reshape:')\n",
    "#print (y_pred [:50])\n",
    "#print (y_pred.shape)\n",
    "#y_train = y_train.argmax (axis = 1)\n",
    "\n",
    "y_pred = clf.predict (X_train)\n",
    "y_pred = y_pred.round ()\n",
    "y_pred = y_pred.reshape (y_pred.shape [0], )\n",
    "print ('\\nPerformance on TRAIN set:')\n",
    "y_pred = clf.predict (X_train)\n",
    "my_confusion_matrix = confusion_matrix (y_train, y_pred,\n",
    "                                        labels = df [TARGET].unique ())\n",
    "tn, fp, fn, tp = my_confusion_matrix.ravel ()\n",
    "### K: NOTE: Scikit's confusion matrix is different from keras. We want attacks to be\n",
    "### the positive class:\n",
    "tp, tn, fp, fn = tn, tp, fn, fp\n",
    "print ('Confusion matrix:')\n",
    "print (my_confusion_matrix)\n",
    "print ('Accuracy:', accuracy_score (y_train, y_pred))\n",
    "print ('Precision:', precision_score (y_train, y_pred, average = 'macro'))\n",
    "print ('Recall:', recall_score (y_train, y_pred, average = 'macro'))\n",
    "print ('F1:', f1_score (y_train, y_pred, average = 'macro'))\n",
    "print ('Cohen Kappa:', cohen_kappa_score (y_train, y_pred,\n",
    "                       labels = df [TARGET].unique ()))\n",
    "print ('TP:', tp)\n",
    "print ('TN:', tn)\n",
    "print ('FP:', fp)\n",
    "print ('FN:', fn)\n",
    "\n",
    "sys.exit ()\n",
    "\n",
    "y_pred = clf.predict (X_test)\n",
    "y_pred = y_pred.round ()\n",
    "y_pred = y_pred.reshape (y_pred.shape [0], )\n",
    "### K: Only before publishing... Don't peek.\n",
    "print ('\\nPerformance on TEST set:')\n",
    "y_pred = clf.predict (X_test_df)\n",
    "my_confusion_matrix = confusion_matrix (y_test, y_pred,\n",
    "                                        labels = df [TARGET].unique ())\n",
    "tn, fp, fn, tp = my_confusion_matrix.ravel ()\n",
    "### K: NOTE: Scikit's confusion matrix is different from keras. We want attacks to be\n",
    "### the positive class:\n",
    "tp, tn, fp, fn = tn, tp, fn, fp\n",
    "print ('Confusion matrix:')\n",
    "print (my_confusion_matrix)\n",
    "print ('Accuracy:', accuracy_score (y_test, y_pred))\n",
    "print ('Precision:', precision_score (y_test, y_pred, average = 'macro'))\n",
    "print ('Recall:', recall_score (y_test, y_pred, average = 'macro'))\n",
    "print ('F1:', f1_score (y_test, y_pred, average = 'macro'))\n",
    "print ('Cohen Kappa:', cohen_kappa_score (y_test, y_pred,\n",
    "                       labels = df [TARGET].unique ()))\n",
    "print ('TP:', tp)\n",
    "print ('TN:', tn)\n",
    "print ('FP:', fp)\n",
    "print ('FN:', fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
