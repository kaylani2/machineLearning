{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kaylani Bochie\n",
    "# github.com/kaylani2\n",
    "# kaylani AT gta DOT ufrj DOT br\n",
    "\n",
    "### K: Model: Decision trees\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from numpy import mean, std\n",
    "from unit import remove_columns_with_one_value, remove_nan_columns, load_dataset\n",
    "from unit import display_general_information, display_feature_distribution\n",
    "from collections import Counter\n",
    "#from imblearn.over_sampling import RandomOverSampler, RandomUnderSampler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif, chi2, mutual_info_classif\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import keras.utils\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Analyse Bezerra's dataset for intrusion detection using Decision Trees\n",
    "###############################################################################\n",
    "\n",
    "###############################################################################\n",
    "## Define constants \n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# Random state for reproducibility\n",
    "try: \n",
    "  # If defined at argv:\n",
    "  STATE = int(sys.argv[1])\n",
    "except:\n",
    "  # If not defined, it will be 0\n",
    "  STATE = 0\n",
    "np.random.seed(10)\n",
    "# List of available attacks on the dataset\n",
    "\n",
    "TARGET = 'Label'\n",
    "\n",
    "\n",
    "# Especific to the repository \n",
    "DATASET_DIRECTORY = r'../../../datasets/Dataset-bezerra-IoT-20200528T203526Z-001/Dataset-IoT/'\n",
    "NETFLOW_DIRECTORY = r'NetFlow/'\n",
    "\n",
    "\n",
    "# There are different csv files on the Dataset, with different types of data:\n",
    "\n",
    "# Some meanings:\n",
    "# MC: Media Center\n",
    "# I: One hour of legitimate and malicious NetFlow data from profile.\n",
    "# L: One hour of legitimate NetFlow data from profile.\n",
    "\n",
    "MC = r'MC/'\n",
    "ST = r'ST/'\n",
    "SC = r'SC/'\n",
    "\n",
    "\n",
    "# MC_I_FIRST: Has infected data by Hajime, Aidra and BashLite botnets \n",
    "MC_I_FIRST = r'MC_I1.csv'\n",
    "\n",
    "# MC_I_SECOND: Has infected data from Mirai botnets\n",
    "MC_I_SECOND = r'MC_I2.csv'\n",
    "\n",
    "# MC_I_THIR: Has infected data from Mirai, Doflo, Tsunami and Wroba botnets\n",
    "MC_I_THIRD = r'MC_I3.csv'\n",
    "\n",
    "# MC_L: Has legitimate data, no infection\n",
    "MC_L = r'MC_L.csv'\n",
    "\n",
    "\n",
    "# Constants for ST\n",
    "ST_I_FIRST = r'ST_I1.csv'\n",
    "ST_I_SECOND = r'ST_I2.csv'\n",
    "ST_I_THIRD = r'ST_I3.csv'\n",
    "ST_L = r'ST_L.csv'\n",
    "\n",
    "# Constants for SC\n",
    "SC_I_FIRST = r'SC_I1.csv'\n",
    "SC_I_SECOND = r'SC_I2.csv'\n",
    "SC_I_THIRD = r'SC_I3.csv'\n",
    "SC_L = r'SC_L.csv'\n",
    "\n",
    "\n",
    "# In[64]:\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Load dataset\n",
    "###############################################################################\n",
    "\n",
    "# For MC data:\n",
    "df_mc_I_first = pd.read_csv (DATASET_DIRECTORY + MC + NETFLOW_DIRECTORY + MC_I_FIRST)\n",
    "df_mc_I_second = pd.read_csv (DATASET_DIRECTORY + MC + NETFLOW_DIRECTORY + MC_I_SECOND)\n",
    "df_mc_I_third = pd.read_csv (DATASET_DIRECTORY + MC + NETFLOW_DIRECTORY + MC_I_THIRD)\n",
    "\n",
    "# Add legitimate rows from MC_L\n",
    "legitimate_frame_mc = pd.read_csv (DATASET_DIRECTORY + MC + NETFLOW_DIRECTORY + MC_L)\n",
    "\n",
    "###################\n",
    "\n",
    "# For ST data:\n",
    "df_st_I_first = pd.read_csv (DATASET_DIRECTORY + ST + NETFLOW_DIRECTORY + ST_I_FIRST)\n",
    "df_st_I_second = pd.read_csv (DATASET_DIRECTORY + ST + NETFLOW_DIRECTORY + ST_I_SECOND)\n",
    "df_st_I_third = pd.read_csv (DATASET_DIRECTORY + ST + NETFLOW_DIRECTORY + ST_I_THIRD)\n",
    "\n",
    "# Add legitimate rows from SC_L\n",
    "legitimate_frame_st = pd.read_csv (DATASET_DIRECTORY + ST + NETFLOW_DIRECTORY + ST_L)\n",
    "\n",
    "\n",
    "###################\n",
    "\n",
    "# For SC data:\n",
    "df_sc_I_first = pd.read_csv (DATASET_DIRECTORY + SC + NETFLOW_DIRECTORY + SC_I_FIRST)\n",
    "df_sc_I_second = pd.read_csv (DATASET_DIRECTORY + SC + NETFLOW_DIRECTORY + SC_I_SECOND)\n",
    "df_sc_I_third = pd.read_csv (DATASET_DIRECTORY + SC + NETFLOW_DIRECTORY + SC_I_THIRD)\n",
    "\n",
    "# Add legitimate rows from MC_L\n",
    "legitimate_frame_sc = pd.read_csv (DATASET_DIRECTORY + SC + NETFLOW_DIRECTORY + SC_L)\n",
    "\n",
    "dataframes_list = [df_mc_I_first,\n",
    "                df_mc_I_second,\n",
    "                df_mc_I_third,\n",
    "                legitimate_frame_mc,\n",
    "                df_st_I_first,\n",
    "                df_st_I_second,\n",
    "                df_st_I_third,\n",
    "                legitimate_frame_st,\n",
    "                df_sc_I_first,\n",
    "                df_sc_I_second,\n",
    "                df_sc_I_third,\n",
    "                legitimate_frame_sc]\n",
    "\n",
    "# Joining the differents DataFrames\n",
    "prev_df = pd.concat(dataframes_list)\n",
    "\n",
    "\n",
    "# In[65]:\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Modify the DataFrame\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "# Sample the dataset if necessary\n",
    "df = prev_df.sample (frac = 1, replace = True, random_state = 0)\n",
    "\n",
    "# We can see that this dataset has a temporal description.\n",
    "# So it is not a good idea to randomly remove rows if using RNN\n",
    "\n",
    "# In this case we drop the index column, since pandas library creates an index\n",
    "# automatically. \n",
    "df = df.drop(df.columns[0], axis=1)\n",
    "\n",
    "# Also drop columns that has no significant data\n",
    "df = df.drop(df.columns[14:], axis=1)\n",
    "\n",
    "# Initial and end time is not a good feature for svm model\n",
    "df = df.drop(['ts', 'te'], axis=1)\n",
    "\n",
    "# Trying another drops to see relation between features and results\n",
    "df = df.drop(['fwd', 'stos', 'sa', 'da'], axis=1)\n",
    "# 'sp', 'dp', 'sa',  'da',  \n",
    "\n",
    "# Counting number of null data\n",
    "nanColumns = [i for i in df.columns if df [i].isnull ().any ()]\n",
    "\n",
    "# Remove NaN and inf values\n",
    "df.replace ('Infinity', np.nan, inplace = True) ## Or other text values\n",
    "df.replace (np.inf, np.nan, inplace = True) ## Remove infinity\n",
    "df.replace (np.nan, 0, inplace = True)\n",
    "\n",
    "\n",
    "# if (df.Label.value_counts()[1] < df.Label.value_counts()[0]):\n",
    "#     remove_n =  df.Label.value_counts()[0] - df.Label.value_counts()[1]  # Number of rows to be removed   \n",
    "#     print(remove_n)\n",
    "#     df_to_be_dropped = df[df.Label == 0]\n",
    "#     drop_indices = np.random.choice(df_to_be_dropped.index, remove_n, replace=False)\n",
    "#     df = df.drop(drop_indices)\n",
    "# else: \n",
    "#     remove_n =  df.Label.value_counts()[1] - df.Label.value_counts()[0]  # Number of rows to be removed   \n",
    "#     print(remove_n)\n",
    "#     df_to_be_dropped = df[df.Label == 1]\n",
    "#     drop_indices = np.random.choice(df_to_be_dropped.index, remove_n, replace=False)\n",
    "#     df = df.drop(drop_indices)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Handling categorical attributes (label encoding).\n",
      "Columns with object types remaining:\n",
      "Objects: []\n"
     ]
    }
   ],
   "source": [
    "print ('\\nHandling categorical attributes (label encoding).')\n",
    "my_label_encoder = LabelEncoder ()\n",
    "df ['flg'] = my_label_encoder.fit_transform (df ['flg'])\n",
    "df ['pr'] = my_label_encoder.fit_transform (df ['pr'])\n",
    "\n",
    "print('Columns with object types remaining:') \n",
    "print ('Objects:', list (df.select_dtypes ( ['object']).columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting dataset (test/train): 0.3\n",
      "X_train_df shape: (1207084, 7)\n",
      "y_train_df shape: (1207084,)\n",
      "X_test_df shape: (517322, 7)\n",
      "y_test_df shape: (517322,)\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Split dataset into train, and test sets\n",
    "###############################################################################\n",
    "### K: Dataset is too big...\n",
    "# drop_indices = np.random.choice (df.index, int (df.shape [0] * 0.5),\n",
    "#                                  replace = False)\n",
    "# df = df.drop (drop_indices)\n",
    "TEST_SIZE = 3/10\n",
    "print ('Splitting dataset (test/train):', TEST_SIZE)\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split (\n",
    "                                               df.iloc [:, 1:],\n",
    "                                               df.iloc [:, 0],\n",
    "                                               test_size = TEST_SIZE,\n",
    "                                               random_state = STATE,)\n",
    "                                               #shuffle = False)\n",
    "\n",
    "print ('X_train_df shape:', X_train_df.shape)\n",
    "print ('y_train_df shape:', y_train_df.shape)\n",
    "print ('X_test_df shape:', X_test_df.shape)\n",
    "print ('y_test_df shape:', y_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.4s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.2s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   2.6s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   2.5s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   2.3s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   2.3s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   2.4s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.5s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.6s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.6s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.6s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.6s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.3s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.4s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.1s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.5s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.6s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.6s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.6s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.5s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.6s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.5s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.6s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.6s\n",
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   0.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed:   50.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   2.0s\n",
      "\n",
      "\n",
      "Best: 0.999733 using {'classifier__criterion': 'entropy', 'classifier__max_depth': 10, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}\n",
      "\n",
      "\n",
      "0.997817 (0.000024) with: {'classifier__criterion': 'gini', 'classifier__max_depth': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}\n",
      "0.997707 (0.000043) with: {'classifier__criterion': 'gini', 'classifier__max_depth': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'random'}\n",
      "0.999626 (0.000024) with: {'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}\n",
      "0.998661 (0.000047) with: {'classifier__criterion': 'gini', 'classifier__max_depth': 10, 'classifier__min_samples_split': 2, 'classifier__splitter': 'random'}\n",
      "0.997666 (0.000001) with: {'classifier__criterion': 'entropy', 'classifier__max_depth': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}\n",
      "0.997666 (0.000001) with: {'classifier__criterion': 'entropy', 'classifier__max_depth': 1, 'classifier__min_samples_split': 2, 'classifier__splitter': 'random'}\n",
      "0.999733 (0.000036) with: {'classifier__criterion': 'entropy', 'classifier__max_depth': 10, 'classifier__min_samples_split': 2, 'classifier__splitter': 'best'}\n",
      "0.998745 (0.000042) with: {'classifier__criterion': 'entropy', 'classifier__max_depth': 10, 'classifier__min_samples_split': 2, 'classifier__splitter': 'random'}\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "### Assemble pipeline for grid search\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "### Define pipeline to scale all the attributes\n",
    "###############################################################################\n",
    "\n",
    "object_features = (list (df.select_dtypes (['object']).columns))\n",
    "remaining_features = list (df.columns)\n",
    "for feature in object_features:\n",
    "    remaining_features.remove (feature)\n",
    "\n",
    "# Remove the target\n",
    "remaining_features.remove ('Label')\n",
    "\n",
    "standard_scaler_features = remaining_features \n",
    "my_scaler = StandardScaler ()\n",
    "steps = list ()\n",
    "steps.append (('scaler', my_scaler))\n",
    "standard_scaler_transformer = Pipeline (steps)\n",
    "\n",
    "preprocessor = ColumnTransformer (transformers = [\n",
    "               ('sca', standard_scaler_transformer, standard_scaler_features)])\n",
    "\n",
    "### Define pipeline to fit the model\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "clf = DecisionTreeClassifier ()\n",
    "clf = Pipeline (steps = [ ('preprocessor', preprocessor),\n",
    "                          ('classifier', clf)],\n",
    "               verbose = True)\n",
    "\n",
    "\n",
    "sorted(clf.get_params().keys())\n",
    "param_grid = {'classifier__criterion' : ['gini', 'entropy'],\n",
    "              'classifier__splitter' : ['best', 'random'],\n",
    "              'classifier__max_depth' : [1, 10],#, 100, None],\n",
    "              'classifier__min_samples_split' : [2]}#, 3, 4]}\n",
    "cv = RepeatedStratifiedKFold (n_splits = 5, n_repeats = 1, random_state = STATE)\n",
    "grid = GridSearchCV (estimator = clf, param_grid = param_grid, scoring = 'f1', verbose = 1, n_jobs = -1, cv = cv)\n",
    "grid_result = grid.fit (X_train_df, y_train_df)\n",
    "\n",
    "print (\"\\n\\nBest: %f using %s\\n\\n\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_['mean_test_score']\n",
    "stds = grid_result.cv_results_['std_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean, stdev, param in zip (means, stds, params):\n",
    "    print (\"%f (%f) with: %r\" % (mean, stdev, param))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "### Define pipeline to scale all the attributes\n",
    "###############################################################################\n",
    "\n",
    "object_features = (list (df.select_dtypes (['object']).columns))\n",
    "remaining_features = list (df.columns)\n",
    "for feature in object_features:\n",
    "    remaining_features.remove (feature)\n",
    "\n",
    "# Remove the target\n",
    "remaining_features.remove ('Label')\n",
    "\n",
    "standard_scaler_features = remaining_features \n",
    "my_scaler = StandardScaler ()\n",
    "steps = list ()\n",
    "steps.append (('scaler', my_scaler))\n",
    "standard_scaler_transformer = Pipeline (steps)\n",
    "\n",
    "preprocessor = ColumnTransformer (transformers = [\n",
    "               ('sca', standard_scaler_transformer, standard_scaler_features)])\n",
    "\n",
    "###############################################################################\n",
    "### Define pipeline for tuned model\n",
    "###############################################################################\n",
    "\n",
    "clf = DecisionTreeClassifier (criterion = 'entropy', max_depth = 10,\n",
    "                              min_samples_split = 2, splitter = 'best')\n",
    "clf = Pipeline (steps = [ ('preprocessor', preprocessor),\n",
    "                          ('classifier', clf)],\n",
    "               verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Pipeline] ...... (step 1 of 2) Processing preprocessor, total=   0.3s\n",
      "[Pipeline] ........ (step 2 of 2) Processing classifier, total=   1.9s\n",
      "2.2577595710754395 s to train model.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "### Fit the model\n",
    "###############################################################################\n",
    "\n",
    "startTime = time.time ()\n",
    "clf = clf.fit (X_train_df, y_train_df)\n",
    "training_time = time.time () - startTime\n",
    "print (str (training_time), 's to train model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision Score:  0.9997203861777567\n",
      "Recall Score:  0.999819398188156\n",
      "Accuracy:  0.9995418714069767\n",
      "F1 Score:  0.9997698897315478\n",
      "Cohen Kappa Score:  0.9493765509783729\n",
      "[[[514852     93]\n",
      "  [   144   2233]]\n",
      "\n",
      " [[  2233    144]\n",
      "  [    93 514852]]]\n"
     ]
    }
   ],
   "source": [
    "from unit import obtain_metrics \n",
    "\n",
    "# Predicting from the test slice\n",
    "y_pred = clf.predict (X_test_df)\n",
    "\n",
    "obtain_metrics(y_test_df, y_pred, \"output_decision_tree_CV.txt\", STATE, training_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
