{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kaylani Bochie\n",
    "# github.com/kaylani2\n",
    "# kaylani AT gta DOT ufrj DOT br\n",
    "\n",
    "### K: Model: Autoencoder\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "sys.path.insert(1, '../')\n",
    "import numpy as np\n",
    "from numpy import mean, std\n",
    "from unit import remove_columns_with_one_value, remove_nan_columns, load_dataset\n",
    "from unit import display_general_information, display_feature_distribution\n",
    "from collections import Counter\n",
    "#from imblearn.over_sampling import RandomOverSampler, RandomUnderSampler\n",
    "import sklearn\n",
    "from sklearn import set_config\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif, chi2, mutual_info_classif\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import keras.utils\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE: 0\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Define constants\n",
    "###############################################################################\n",
    "pd.set_option ('display.max_rows', None)\n",
    "pd.set_option ('display.max_columns', 5)\n",
    "BOT_IOT_DIRECTORY = '../../../../../datasets/bot-iot/'\n",
    "BOT_IOT_FEATURE_NAMES = 'UNSW_2018_IoT_Botnet_Dataset_Feature_Names.csv'\n",
    "BOT_IOT_FILE_5_PERCENT_SCHEMA = 'UNSW_2018_IoT_Botnet_Full5pc_{}.csv' # 1 - 4\n",
    "FIVE_PERCENT_FILES = 4\n",
    "BOT_IOT_FILE_FULL_SCHEMA = 'UNSW_2018_IoT_Botnet_Dataset_{}.csv' # 1 - 74\n",
    "FULL_FILES = 74\n",
    "FILE_NAME = BOT_IOT_DIRECTORY + BOT_IOT_FILE_5_PERCENT_SCHEMA\n",
    "FEATURES = BOT_IOT_DIRECTORY + BOT_IOT_FEATURE_NAMES\n",
    "NAN_VALUES = ['?', '.']\n",
    "TARGET = 'attack'\n",
    "INDEX_COLUMN = 'pkSeqID'\n",
    "LABELS = ['attack', 'category', 'subcategory']\n",
    "STATE = 0\n",
    "try:\n",
    "  STATE = int (sys.argv [1])\n",
    "except:\n",
    "  pass\n",
    "#for STATE in [1, 2, 3, 4, 5]:\n",
    "np.random.seed (STATE)\n",
    "print ('STATE:', STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../../../../../datasets/bot-iot/UNSW_2018_IoT_Botnet_Full5pc_1.csv\n",
      "Reading ../../../../../datasets/bot-iot/UNSW_2018_IoT_Botnet_Full5pc_2.csv\n",
      "Reading ../../../../../datasets/bot-iot/UNSW_2018_IoT_Botnet_Full5pc_3.csv\n",
      "Reading ../../../../../datasets/bot-iot/UNSW_2018_IoT_Botnet_Full5pc_4.csv\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Load dataset\n",
    "###############################################################################\n",
    "df = load_dataset (FILE_NAME, FIVE_PERCENT_FILES, INDEX_COLUMN, NAN_VALUES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While removing single value columns: No columns dropped.\n",
      "Removing redundant columns: ['state_number', 'proto_number', 'flgs_number']\n",
      "Removing useless targets: ['category', 'subcategory']\n",
      "Removing misc columns: ['saddr', 'daddr']\n",
      "While removing nan value columns: No columns dropped.\n",
      "Encoding categorical features (ordinal encoding).\n",
      "Objects: []\n",
      "Dataframe shape (lines, columns): (3668522, 38) \n",
      "\n",
      "First 5 entries:\n",
      "                 stime  flgs  ...  Pkts_P_State_P_Protocol_P_SrcIP  attack\n",
      "pkSeqID                      ...                                         \n",
      "1        1.528089e+09   0.0  ...                              602       1\n",
      "2        1.528089e+09   0.0  ...                                6       1\n",
      "3        1.528089e+09   0.0  ...                              602       1\n",
      "4        1.528089e+09   0.0  ...                              602       1\n",
      "5        1.528089e+09   0.0  ...                              602       1\n",
      "\n",
      "[5 rows x 38 columns] \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 3668522 entries, 1 to 3668522\n",
      "Data columns (total 38 columns):\n",
      " #   Column                            Dtype  \n",
      "---  ------                            -----  \n",
      " 0   stime                             float64\n",
      " 1   flgs                              float64\n",
      " 2   proto                             float64\n",
      " 3   sport                             float64\n",
      " 4   dport                             float64\n",
      " 5   pkts                              int64  \n",
      " 6   bytes                             int64  \n",
      " 7   state                             float64\n",
      " 8   ltime                             float64\n",
      " 9   seq                               int64  \n",
      " 10  dur                               float64\n",
      " 11  mean                              float64\n",
      " 12  stddev                            float64\n",
      " 13  sum                               float64\n",
      " 14  min                               float64\n",
      " 15  max                               float64\n",
      " 16  spkts                             int64  \n",
      " 17  dpkts                             int64  \n",
      " 18  sbytes                            int64  \n",
      " 19  dbytes                            int64  \n",
      " 20  rate                              float64\n",
      " 21  srate                             float64\n",
      " 22  drate                             float64\n",
      " 23  TnBPSrcIP                         int64  \n",
      " 24  TnBPDstIP                         int64  \n",
      " 25  TnP_PSrcIP                        int64  \n",
      " 26  TnP_PDstIP                        int64  \n",
      " 27  TnP_PerProto                      int64  \n",
      " 28  TnP_Per_Dport                     int64  \n",
      " 29  AR_P_Proto_P_SrcIP                float64\n",
      " 30  AR_P_Proto_P_DstIP                float64\n",
      " 31  N_IN_Conn_P_DstIP                 int64  \n",
      " 32  N_IN_Conn_P_SrcIP                 int64  \n",
      " 33  AR_P_Proto_P_Sport                float64\n",
      " 34  AR_P_Proto_P_Dport                float64\n",
      " 35  Pkts_P_State_P_Protocol_P_DestIP  int64  \n",
      " 36  Pkts_P_State_P_Protocol_P_SrcIP   int64  \n",
      " 37  attack                            int64  \n",
      "dtypes: float64(20), int64(18)\n",
      "memory usage: 1.1 GB\n",
      "Brief description:\n",
      "\n",
      "Dataframe contains NaN values: False\n",
      "Number of NaN columns: 0\n",
      "NaN columns: [] \n",
      "\n",
      "\n",
      "Column | # of different values\n",
      "stime                                        392259  \n",
      "flgs                                              9  \n",
      "proto                                             5  \n",
      "sport                                         65541  \n",
      "dport                                          7698  \n",
      "pkts                                            123  \n",
      "bytes                                          1633  \n",
      "state                                            11  \n",
      "ltime                                        383624  \n",
      "seq                                          262212  \n",
      "dur                                          612509  \n",
      "mean                                         507089  \n",
      "stddev                                       421379  \n",
      "sum                                          934972  \n",
      "min                                          271147  \n",
      "max                                          594525  \n",
      "spkts                                            91  \n",
      "dpkts                                            62  \n",
      "sbytes                                         1052  \n",
      "dbytes                                          472  \n",
      "rate                                         139677  \n",
      "srate                                        119709  \n",
      "drate                                         20714  \n",
      "TnBPSrcIP                                      8639  \n",
      "TnBPDstIP                                      7631  \n",
      "TnP_PSrcIP                                     1522  \n",
      "TnP_PDstIP                                     1587  \n",
      "TnP_PerProto                                   1560  \n",
      "TnP_Per_Dport                                  1582  \n",
      "AR_P_Proto_P_SrcIP                            46289  \n",
      "AR_P_Proto_P_DstIP                            39186  \n",
      "N_IN_Conn_P_DstIP                               100  \n",
      "N_IN_Conn_P_SrcIP                               100  \n",
      "AR_P_Proto_P_Sport                           136207  \n",
      "AR_P_Proto_P_Dport                            42237  \n",
      "Pkts_P_State_P_Protocol_P_DestIP               1595  \n",
      "Pkts_P_State_P_Protocol_P_SrcIP                1526  \n",
      "attack                                            2  \n",
      "\n",
      "stime                                        392259  \n",
      "flgs [0. 7. 5. 4. 6. 8. 3. 1. 2.]\n",
      "proto [3. 0. 4. 1. 2.]\n",
      "sport                                         65541  \n",
      "dport                                          7698  \n",
      "pkts                                            123  \n",
      "bytes                                          1633  \n",
      "state                                            11  \n",
      "ltime                                        383624  \n",
      "seq                                          262212  \n",
      "dur                                          612509  \n",
      "mean                                         507089  \n",
      "stddev                                       421379  \n",
      "sum                                          934972  \n",
      "min                                          271147  \n",
      "max                                          594525  \n",
      "spkts                                            91  \n",
      "dpkts                                            62  \n",
      "sbytes                                         1052  \n",
      "dbytes                                          472  \n",
      "rate                                         139677  \n",
      "srate                                        119709  \n",
      "drate                                         20714  \n",
      "TnBPSrcIP                                      8639  \n",
      "TnBPDstIP                                      7631  \n",
      "TnP_PSrcIP                                     1522  \n",
      "TnP_PDstIP                                     1587  \n",
      "TnP_PerProto                                   1560  \n",
      "TnP_Per_Dport                                  1582  \n",
      "AR_P_Proto_P_SrcIP                            46289  \n",
      "AR_P_Proto_P_DstIP                            39186  \n",
      "N_IN_Conn_P_DstIP                               100  \n",
      "N_IN_Conn_P_SrcIP                               100  \n",
      "AR_P_Proto_P_Sport                           136207  \n",
      "AR_P_Proto_P_Dport                            42237  \n",
      "Pkts_P_State_P_Protocol_P_DestIP               1595  \n",
      "Pkts_P_State_P_Protocol_P_SrcIP                1526  \n",
      "attack [1 0]\n",
      "\n",
      "Objects: (select encoding method)\n",
      "\n",
      "Check for high cardinality.\n",
      "Column | # of different values | values\n",
      "Objects: [] \n",
      "\n",
      "Attack set:\n",
      "1    1834017\n",
      "Name: attack, dtype: int64\n",
      "Normal set:\n",
      "0    244\n",
      "Name: attack, dtype: int64\n",
      "Test set:\n",
      "1    244\n",
      "0    244\n",
      "Name: attack, dtype: int64\n",
      "\n",
      "Splitting dataset (validation/train): 0.25\n",
      "X_train_df shape: (1375695, 37)\n",
      "y_train_df shape: (1375695,)\n",
      "X_val_df shape: (458566, 37)\n",
      "y_val_df shape: (458566,)\n",
      "X_test_df shape: (488, 37)\n",
      "y_test_df shape: (488,)\n",
      "\n",
      "Converting dataframe to numpy array.\n",
      "X_train shape: (1375695, 37)\n",
      "y_train shape: (1375695,)\n",
      "X_val shape: (458566, 37)\n",
      "y_val shape: (458566,)\n",
      "X_test shape: (488, 37)\n",
      "y_test shape: (488,)\n",
      "\n",
      "Applying normalization.\n",
      "1.020211935043335 to normalize data.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Clean dataset\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "### Remove columns with only one value\n",
    "df, log = remove_columns_with_one_value (df, verbose = False)\n",
    "print (log)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "### Remove redundant columns, useless columns and unused targets\n",
    "### K: _number columns are numerical representations of other existing columns.\n",
    "### K: category and subcategory are other labels.\n",
    "### K: saddr and daddr may specialize the model to a single network\n",
    "redundant_columns = ['state_number', 'proto_number', 'flgs_number']\n",
    "other_targets = ['category', 'subcategory']\n",
    "misc_columns = ['saddr', 'daddr']\n",
    "print ('Removing redundant columns:', redundant_columns)\n",
    "print ('Removing useless targets:', other_targets)\n",
    "print ('Removing misc columns:', misc_columns)\n",
    "columns_to_remove = redundant_columns + other_targets + misc_columns\n",
    "df.drop (axis = 'columns', columns = columns_to_remove, inplace = True)\n",
    "\n",
    "###############################################################################\n",
    "### Remove NaN columns (with a lot of NaN values)\n",
    "df, log = remove_nan_columns (df, 1/2, verbose = False)\n",
    "print (log)\n",
    "\n",
    "###############################################################################\n",
    "### Encode categorical features\n",
    "print ('Encoding categorical features (ordinal encoding).')\n",
    "my_encoder = OrdinalEncoder ()\n",
    "df ['flgs'] = my_encoder.fit_transform (df ['flgs'].values.reshape (-1, 1))\n",
    "df ['proto'] = my_encoder.fit_transform (df ['proto'].values.reshape (-1, 1))\n",
    "df ['sport'] = my_encoder.fit_transform (df ['sport'].astype (str).values.reshape (-1, 1))\n",
    "df ['dport'] = my_encoder.fit_transform (df ['dport'].astype (str).values.reshape (-1, 1))\n",
    "df ['state'] = my_encoder.fit_transform (df ['state'].values.reshape (-1, 1))\n",
    "print ('Objects:', list (df.select_dtypes ( ['object']).columns))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Quick sanity check\n",
    "###############################################################################\n",
    "display_general_information (df)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Split dataset into train, validation and test sets\n",
    "###############################################################################\n",
    "### Isolate attack and normal samples\n",
    "## K: Dataset is too big? Drop.\n",
    "drop_indices = np.random.choice (df.index, int (df.shape [0] * 0.5),\n",
    "                                 replace = False)\n",
    "df = df.drop (drop_indices)\n",
    "mask = df [TARGET] == 0\n",
    "# 0 == normal\n",
    "df_normal = df [mask]\n",
    "# 1 == attack\n",
    "df_attack = df [~mask]\n",
    "\n",
    "print ('Attack set:')\n",
    "print (df_attack [TARGET].value_counts ())\n",
    "print ('Normal set:')\n",
    "print (df_normal [TARGET].value_counts ())\n",
    "\n",
    "### Sample and drop random attacks\n",
    "df_random_attacks = df_attack.sample (n = df_normal.shape [0], random_state = STATE)\n",
    "df_attack = df_attack.drop (df_random_attacks.index)\n",
    "\n",
    "### Assemble test set\n",
    "df_test = pd.DataFrame ()\n",
    "df_test = pd.concat ( [df_test, df_normal])\n",
    "df_test = pd.concat ( [df_test, df_random_attacks])\n",
    "print ('Test set:')\n",
    "print (df_test [TARGET].value_counts ())\n",
    "X_test_df = df_test.iloc [:, :-1]\n",
    "y_test_df = df_test.iloc [:, -1]\n",
    "### K: y_test is required to plot the roc curve in the end\n",
    "\n",
    "\n",
    "\n",
    "df_train = df_attack\n",
    "VALIDATION_SIZE = 1/4\n",
    "print ('\\nSplitting dataset (validation/train):', VALIDATION_SIZE)\n",
    "X_train_df, X_val_df, y_train_df, y_val_df = train_test_split (\n",
    "                                             df.loc [:, df.columns != TARGET],\n",
    "                                             df [TARGET],\n",
    "                                             test_size = VALIDATION_SIZE,\n",
    "                                             random_state = STATE,)\n",
    "\n",
    "\n",
    "print ('X_train_df shape:', X_train_df.shape)\n",
    "print ('y_train_df shape:', y_train_df.shape)\n",
    "print ('X_val_df shape:', X_val_df.shape)\n",
    "print ('y_val_df shape:', y_val_df.shape)\n",
    "print ('X_test_df shape:', X_test_df.shape)\n",
    "print ('y_test_df shape:', y_test_df.shape)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Convert dataframe to a numpy array\n",
    "###############################################################################\n",
    "print ('\\nConverting dataframe to numpy array.')\n",
    "X_train = X_train_df.values\n",
    "y_train = y_train_df.values\n",
    "X_val = X_val_df.values\n",
    "y_val = y_val_df.values\n",
    "X_test = X_test_df.values\n",
    "y_test = y_test_df.values\n",
    "print ('X_train shape:', X_train.shape)\n",
    "print ('y_train shape:', y_train.shape)\n",
    "print ('X_val shape:', X_val.shape)\n",
    "print ('y_val shape:', y_val.shape)\n",
    "print ('X_test shape:', X_test.shape)\n",
    "print ('y_test shape:', y_test.shape)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Apply normalization\n",
    "###############################################################################\n",
    "### K: NOTE: Only use derived information from the train set to avoid leakage.\n",
    "print ('\\nApplying normalization.')\n",
    "startTime = time.time ()\n",
    "scaler = StandardScaler ()\n",
    "scaler.fit (X_train)\n",
    "X_train = scaler.transform (X_train)\n",
    "X_val = scaler.transform (X_val)\n",
    "X_test = scaler.transform (X_test)\n",
    "print (str (time.time () - startTime), 'to normalize data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selecting top 9 features.\n",
      "0.6843316555023193 to select features.\n",
      "X_train shape: (1375695, 9)\n",
      "y_train shape: (1375695,)\n",
      "X_val shape: (458566, 9)\n",
      "y_val shape: (458566,)\n",
      "X_test shape: (488, 9)\n",
      "y_test shape: (488,)\n",
      "Feature 33: 0.150036\n",
      "Feature 34: 0.214839\n",
      "Feature 29: 0.346984\n",
      "Feature 3: 0.375641\n",
      "Feature 20: 0.980276\n",
      "Feature 2: 2.823311\n",
      "Feature 30: 14.412533\n",
      "Feature 22: 24.328652\n",
      "Feature 14: 35.440986\n",
      "Feature 1: 103.171713\n",
      "Feature 12: 199.838927\n",
      "Feature 11: 217.378814\n",
      "Feature 15: 302.776711\n",
      "Feature 9: 399.088130\n",
      "Feature 7: 607.828276\n",
      "Feature 32: 1291.452807\n",
      "Feature 4: 2120.666061\n",
      "Feature 31: 3894.588067\n",
      "Feature 8: 7904.506295\n",
      "Feature 0: 7906.483412\n",
      "Feature 36: 12266.165069\n",
      "Feature 10: 17757.876505\n",
      "Feature 21: 17985.356873\n",
      "Feature 35: 21023.029310\n",
      "Feature 19: 38196.723647\n",
      "Feature 17: 38454.644699\n",
      "Feature 28: 54658.777094\n",
      "Feature 13: 56684.287133\n",
      "Feature 6: 75197.671342\n",
      "Feature 26: 79907.799271\n",
      "Feature 18: 91032.764024\n",
      "Feature 5: 96362.005593\n",
      "Feature 16: 119512.654387\n",
      "Feature 25: 121962.773377\n",
      "Feature 24: 127332.735963\n",
      "Feature 23: 179329.928409\n",
      "Feature 27: 2798193.337192\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Perform feature selection\n",
    "###############################################################################\n",
    "### K: Let the autoencoder reconstruct the data.\n",
    "###############################################################################\n",
    "NUMBER_OF_FEATURES = 9 #'all'\n",
    "print ('\\nSelecting top', NUMBER_OF_FEATURES, 'features.')\n",
    "startTime = time.time ()\n",
    "#fs = SelectKBest (score_func = mutual_info_classif, k = NUMBER_OF_FEATURES)\n",
    "### K: ~30 minutes to FAIL fit mutual_info_classif to 5% bot-iot\n",
    "#fs = SelectKBest (score_func = chi2, k = NUMBER_OF_FEATURES) # X must be >= 0\n",
    "### K: ~4 seconds to fit chi2 to 5% bot-iot (MinMaxScaler (0, 1))\n",
    "fs = SelectKBest (score_func = f_classif, k = NUMBER_OF_FEATURES)\n",
    "### K: ~4 seconds to fit f_classif to 5% bot-iot\n",
    "fs.fit (X_train, y_train)\n",
    "X_train = fs.transform (X_train)\n",
    "X_val = fs.transform (X_val)\n",
    "X_test = fs.transform (X_test)\n",
    "print (str (time.time () - startTime), 'to select features.')\n",
    "print ('X_train shape:', X_train.shape)\n",
    "print ('y_train shape:', y_train.shape)\n",
    "print ('X_val shape:', X_val.shape)\n",
    "print ('y_val shape:', y_val.shape)\n",
    "print ('X_test shape:', X_test.shape)\n",
    "print ('y_test shape:', y_test.shape)\n",
    "bestFeatures = []\n",
    "for feature in range (len (fs.scores_)):\n",
    "  bestFeatures.append ({'f': feature, 's': fs.scores_ [feature]})\n",
    "  bestFeatures = sorted (bestFeatures, key = lambda k: k ['s'])\n",
    "for feature in bestFeatures:\n",
    "  print ('Feature %d: %f' % (feature ['f'], feature ['s']))\n",
    "\n",
    "#pyplot.bar ( [i for i in range (len (fs.scores_))], fs.scores_)\n",
    "#pyplot.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1 folds for each of 6 candidates, totalling 6 fits\n",
      "[CV] batch_size=5000, dropout_rate=0.0, epochs=10, learn_rate=0.001, weight_constraint=0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "276/276 - 1s - loss: 0.5948 - mse: 0.5948\n",
      "Epoch 2/10\n",
      "276/276 - 1s - loss: 0.1812 - mse: 0.1812\n",
      "Epoch 3/10\n",
      "276/276 - 1s - loss: 0.1458 - mse: 0.1458\n",
      "Epoch 4/10\n",
      "276/276 - 1s - loss: 0.1268 - mse: 0.1268\n",
      "Epoch 5/10\n",
      "276/276 - 1s - loss: 0.0917 - mse: 0.0917\n",
      "Epoch 6/10\n",
      "276/276 - 1s - loss: 0.0747 - mse: 0.0747\n",
      "Epoch 7/10\n",
      "276/276 - 1s - loss: 0.0633 - mse: 0.0633\n",
      "Epoch 8/10\n",
      "276/276 - 1s - loss: 0.0482 - mse: 0.0482\n",
      "Epoch 9/10\n",
      "276/276 - 1s - loss: 0.0375 - mse: 0.0375\n",
      "Epoch 10/10\n",
      "276/276 - 1s - loss: 0.0400 - mse: 0.0400\n",
      "92/92 - 0s\n",
      "[CV]  batch_size=5000, dropout_rate=0.0, epochs=10, learn_rate=0.001, weight_constraint=0, total=  10.9s\n",
      "[CV] batch_size=5000, dropout_rate=0.0, epochs=10, learn_rate=0.01, weight_constraint=0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   10.9s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "276/276 - 1s - loss: 0.5971 - mse: 0.5971\n",
      "Epoch 2/10\n",
      "276/276 - 1s - loss: 0.1893 - mse: 0.1893\n",
      "Epoch 3/10\n",
      "276/276 - 1s - loss: 0.1544 - mse: 0.1544\n",
      "Epoch 4/10\n",
      "276/276 - 1s - loss: 0.1196 - mse: 0.1196\n",
      "Epoch 5/10\n",
      "276/276 - 1s - loss: 0.0969 - mse: 0.0969\n",
      "Epoch 6/10\n",
      "276/276 - 1s - loss: 0.0764 - mse: 0.0764\n",
      "Epoch 7/10\n",
      "276/276 - 1s - loss: 0.0515 - mse: 0.0515\n",
      "Epoch 8/10\n",
      "276/276 - 1s - loss: 0.0386 - mse: 0.0386\n",
      "Epoch 9/10\n",
      "276/276 - 1s - loss: 0.0390 - mse: 0.0390\n",
      "Epoch 10/10\n",
      "276/276 - 1s - loss: 0.0311 - mse: 0.0311\n",
      "92/92 - 0s\n",
      "[CV]  batch_size=5000, dropout_rate=0.0, epochs=10, learn_rate=0.01, weight_constraint=0, total=  10.5s\n",
      "[CV] batch_size=5000, dropout_rate=0.0, epochs=10, learn_rate=0.1, weight_constraint=0 \n",
      "Epoch 1/10\n",
      "276/276 - 1s - loss: 0.7056 - mse: 0.7056\n",
      "Epoch 2/10\n",
      "276/276 - 1s - loss: 0.2086 - mse: 0.2086\n",
      "Epoch 3/10\n",
      "276/276 - 1s - loss: 0.1164 - mse: 0.1164\n",
      "Epoch 4/10\n",
      "276/276 - 1s - loss: 0.0874 - mse: 0.0874\n",
      "Epoch 5/10\n",
      "276/276 - 1s - loss: 0.0469 - mse: 0.0469\n",
      "Epoch 6/10\n",
      "276/276 - 1s - loss: 0.0332 - mse: 0.0332\n",
      "Epoch 7/10\n",
      "276/276 - 1s - loss: 0.0266 - mse: 0.0266\n",
      "Epoch 8/10\n",
      "276/276 - 1s - loss: 0.0210 - mse: 0.0210\n",
      "Epoch 9/10\n",
      "276/276 - 1s - loss: 0.0226 - mse: 0.0226\n",
      "Epoch 10/10\n",
      "276/276 - 1s - loss: 0.0211 - mse: 0.0211\n",
      "92/92 - 0s\n",
      "[CV]  batch_size=5000, dropout_rate=0.0, epochs=10, learn_rate=0.1, weight_constraint=0, total=  10.5s\n",
      "[CV] batch_size=10000, dropout_rate=0.0, epochs=10, learn_rate=0.001, weight_constraint=0 \n",
      "Epoch 1/10\n",
      "138/138 - 1s - loss: 0.9153 - mse: 0.9153\n",
      "Epoch 2/10\n",
      "138/138 - 1s - loss: 0.3422 - mse: 0.3422\n",
      "Epoch 3/10\n",
      "138/138 - 1s - loss: 0.1387 - mse: 0.1387\n",
      "Epoch 4/10\n",
      "138/138 - 1s - loss: 0.0961 - mse: 0.0961\n",
      "Epoch 5/10\n",
      "138/138 - 1s - loss: 0.0836 - mse: 0.0836\n",
      "Epoch 6/10\n",
      "138/138 - 1s - loss: 0.0620 - mse: 0.0620\n",
      "Epoch 7/10\n",
      "138/138 - 1s - loss: 0.0503 - mse: 0.0503\n",
      "Epoch 8/10\n",
      "138/138 - 1s - loss: 0.0394 - mse: 0.0394\n",
      "Epoch 9/10\n",
      "138/138 - 1s - loss: 0.0322 - mse: 0.0322\n",
      "Epoch 10/10\n",
      "138/138 - 1s - loss: 0.0282 - mse: 0.0282\n",
      "46/46 - 0s\n",
      "[CV]  batch_size=10000, dropout_rate=0.0, epochs=10, learn_rate=0.001, weight_constraint=0, total=   8.2s\n",
      "[CV] batch_size=10000, dropout_rate=0.0, epochs=10, learn_rate=0.01, weight_constraint=0 \n",
      "Epoch 1/10\n",
      "138/138 - 1s - loss: 0.9398 - mse: 0.9398\n",
      "Epoch 2/10\n",
      "138/138 - 1s - loss: 0.6277 - mse: 0.6277\n",
      "Epoch 3/10\n",
      "138/138 - 1s - loss: 0.2134 - mse: 0.2134\n",
      "Epoch 4/10\n",
      "138/138 - 1s - loss: 0.1422 - mse: 0.1422\n",
      "Epoch 5/10\n",
      "138/138 - 1s - loss: 0.1139 - mse: 0.1139\n",
      "Epoch 6/10\n",
      "138/138 - 1s - loss: 0.1038 - mse: 0.1038\n",
      "Epoch 7/10\n",
      "138/138 - 1s - loss: 0.0791 - mse: 0.0791\n",
      "Epoch 8/10\n",
      "138/138 - 1s - loss: 0.0542 - mse: 0.0542\n",
      "Epoch 9/10\n",
      "138/138 - 1s - loss: 0.0418 - mse: 0.0418\n",
      "Epoch 10/10\n",
      "138/138 - 1s - loss: 0.0282 - mse: 0.0282\n",
      "46/46 - 0s\n",
      "[CV]  batch_size=10000, dropout_rate=0.0, epochs=10, learn_rate=0.01, weight_constraint=0, total=   8.3s\n",
      "[CV] batch_size=10000, dropout_rate=0.0, epochs=10, learn_rate=0.1, weight_constraint=0 \n",
      "Epoch 1/10\n",
      "138/138 - 1s - loss: 0.8171 - mse: 0.8171\n",
      "Epoch 2/10\n",
      "138/138 - 1s - loss: 0.3344 - mse: 0.3344\n",
      "Epoch 3/10\n",
      "138/138 - 1s - loss: 0.1814 - mse: 0.1814\n",
      "Epoch 4/10\n",
      "138/138 - 1s - loss: 0.1192 - mse: 0.1192\n",
      "Epoch 5/10\n",
      "138/138 - 1s - loss: 0.0979 - mse: 0.0979\n",
      "Epoch 6/10\n",
      "138/138 - 1s - loss: 0.0857 - mse: 0.0857\n",
      "Epoch 7/10\n",
      "138/138 - 1s - loss: 0.0787 - mse: 0.0787\n",
      "Epoch 8/10\n",
      "138/138 - 1s - loss: 0.0629 - mse: 0.0629\n",
      "Epoch 9/10\n",
      "138/138 - 1s - loss: 0.0533 - mse: 0.0533\n",
      "Epoch 10/10\n",
      "138/138 - 1s - loss: 0.0445 - mse: 0.0445\n",
      "46/46 - 0s\n",
      "[CV]  batch_size=10000, dropout_rate=0.0, epochs=10, learn_rate=0.1, weight_constraint=0, total=   7.9s\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   6 out of   6 | elapsed:   56.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "367/367 - 1s - loss: 0.4553 - mse: 0.4553\n",
      "Epoch 2/10\n",
      "367/367 - 1s - loss: 0.1246 - mse: 0.1246\n",
      "Epoch 3/10\n",
      "367/367 - 1s - loss: 0.0855 - mse: 0.0855\n",
      "Epoch 4/10\n",
      "367/367 - 1s - loss: 0.0670 - mse: 0.0670\n",
      "Epoch 5/10\n",
      "367/367 - 1s - loss: 0.0550 - mse: 0.0550\n",
      "Epoch 6/10\n",
      "367/367 - 1s - loss: 0.0470 - mse: 0.0470\n",
      "Epoch 7/10\n",
      "367/367 - 1s - loss: 0.0396 - mse: 0.0396\n",
      "Epoch 8/10\n",
      "367/367 - 1s - loss: 0.0338 - mse: 0.0338\n",
      "Epoch 9/10\n",
      "367/367 - 1s - loss: 0.0273 - mse: 0.0273\n",
      "Epoch 10/10\n",
      "367/367 - 1s - loss: 0.0297 - mse: 0.0297\n",
      "{'batch_size': 5000, 'dropout_rate': 0.0, 'epochs': 10, 'learn_rate': 0.1, 'weight_constraint': 0}\n",
      "Best: -0.015576 using {'batch_size': 5000, 'dropout_rate': 0.0, 'epochs': 10, 'learn_rate': 0.1, 'weight_constraint': 0}\n",
      "-0.062872 (0.000000) with: {'batch_size': 5000, 'dropout_rate': 0.0, 'epochs': 10, 'learn_rate': 0.001, 'weight_constraint': 0}\n",
      "-0.032232 (0.000000) with: {'batch_size': 5000, 'dropout_rate': 0.0, 'epochs': 10, 'learn_rate': 0.01, 'weight_constraint': 0}\n",
      "-0.015576 (0.000000) with: {'batch_size': 5000, 'dropout_rate': 0.0, 'epochs': 10, 'learn_rate': 0.1, 'weight_constraint': 0}\n",
      "-0.053655 (0.000000) with: {'batch_size': 10000, 'dropout_rate': 0.0, 'epochs': 10, 'learn_rate': 0.001, 'weight_constraint': 0}\n",
      "-0.038956 (0.000000) with: {'batch_size': 10000, 'dropout_rate': 0.0, 'epochs': 10, 'learn_rate': 0.01, 'weight_constraint': 0}\n",
      "-0.049877 (0.000000) with: {'batch_size': 10000, 'dropout_rate': 0.0, 'epochs': 10, 'learn_rate': 0.1, 'weight_constraint': 0}\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Create learning model (Autoencoder) and tune hyperparameters\n",
    "###############################################################################\n",
    "\n",
    "###############################################################################\n",
    "#Hyperparameter tuning\n",
    "test_fold = np.repeat ( [-1, 0], [X_train.shape [0], X_val.shape [0]])\n",
    "myPreSplit = PredefinedSplit (test_fold)\n",
    "def create_model (learn_rate = 0.01, dropout_rate = 0.0, weight_constraint = 0,\n",
    "                  metrics = ['mse']):\n",
    " model = Sequential ()\n",
    " model.add (Dense (X_train.shape [1], activation = 'relu',\n",
    "                   input_shape = (X_train.shape [1], )))\n",
    " model.add (Dense (32, activation = 'relu'))\n",
    " model.add (Dense (8,  activation = 'relu'))\n",
    " model.add (Dense (32, activation = 'relu'))\n",
    " model.add (Dense (X_train.shape [1], activation = None))\n",
    " model.compile (loss = 'mean_squared_error',\n",
    "                optimizer = 'adam',\n",
    "                metrics = metrics)\n",
    " return model\n",
    "\n",
    "\n",
    "model = KerasRegressor (build_fn = create_model, verbose = 2)\n",
    "batch_size = [5000, 10000]#, 50]\n",
    "epochs = [10]#, 5, 10]\n",
    "learn_rate = [0.001, 0.01, 0.1]#, 0.2, 0.3]\n",
    "dropout_rate = [0.0]#, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "weight_constraint = [0]#1, 2, 3, 4, 5]\n",
    "param_grid = dict (batch_size = batch_size, epochs = epochs,\n",
    "                  dropout_rate = dropout_rate, learn_rate = learn_rate,\n",
    "                  weight_constraint = weight_constraint)\n",
    "grid = GridSearchCV (estimator = model, param_grid = param_grid,\n",
    "                    scoring = 'neg_mean_squared_error', cv = myPreSplit,\n",
    "                    verbose = 2, n_jobs = 1)\n",
    "\n",
    "grid_result = grid.fit (np.vstack ( (X_train, X_val)),#, axis = 1),\n",
    "                       np.vstack ( (X_train, X_val)))#, axis = 1))\n",
    "print (grid_result.best_params_)\n",
    "\n",
    "print (\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_ ['mean_test_score']\n",
    "stds = grid_result.cv_results_ ['std_test_score']\n",
    "params = grid_result.cv_results_ ['params']\n",
    "for mean, stdev, param in zip (means, stds, params):\n",
    "  print (\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# Best: -0.129429 using {'batch_size': 30, 'dropout_rate': 0.0, 'epochs': 5, 'learn_rate': 0.1, 'weight_constraint': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating learning model.\n",
      "\n",
      "Compiling the network.\n",
      "Model summary:\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_120 (Dense)            (None, 9)                 90        \n",
      "_________________________________________________________________\n",
      "dense_121 (Dense)            (None, 32)                320       \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 9)                 297       \n",
      "=================================================================\n",
      "Total params: 1,259\n",
      "Trainable params: 1,259\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Finished model\n",
    "# Best: -0.015576 using {'batch_size': 5000, 'dropout_rate': 0.0, 'epochs': 10,\n",
    "# learn_rate': 0.1, 'weight_constraint': 0}\n",
    "METRICS = [keras.metrics.MeanSquaredError (name = 'MSE'),\n",
    "           keras.metrics.RootMeanSquaredError (name = 'RMSE'),  \n",
    "           keras.metrics.MeanAbsoluteError (name = 'MAE'),]\n",
    "### K: learning rate foi alterado manualmente ao olhar os valores do erro na\n",
    "### validacao ao longo das epochs...\n",
    "NUMBER_OF_EPOCHS = 25\n",
    "BATCH_SIZE = 5000\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "print ('\\nCreating learning model.')\n",
    "clf = Sequential ()\n",
    "clf.add (Dense (X_train.shape [1], activation = 'relu',\n",
    "                      input_shape = (X_train.shape [1], )))\n",
    "clf.add (Dense (32, activation = 'relu'))\n",
    "clf.add (Dense (8,  activation = 'relu'))\n",
    "clf.add (Dense (32, activation = 'relu'))\n",
    "clf.add (Dense (X_train.shape [1], activation = None))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Compile the network\n",
    "###############################################################################\n",
    "print ('\\nCompiling the network.')\n",
    "clf.compile (loss = 'mean_squared_error',\n",
    "                   optimizer = Adam (lr = LEARNING_RATE),\n",
    "                   metrics = METRICS)\n",
    "print ('Model summary:')\n",
    "clf.summary ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting the network.\n",
      "Epoch 1/30\n",
      "276/276 - 1s - loss: 0.7954 - MSE: 0.7954 - RMSE: 0.8918 - MAE: 0.1024 - val_loss: 0.2594 - val_MSE: 0.2594 - val_RMSE: 0.5094 - val_MAE: 0.0601\n",
      "Epoch 2/30\n",
      "276/276 - 1s - loss: 0.2538 - MSE: 0.2538 - RMSE: 0.5038 - MAE: 0.0602 - val_loss: 0.1175 - val_MSE: 0.1175 - val_RMSE: 0.3428 - val_MAE: 0.0645\n",
      "Epoch 3/30\n",
      "276/276 - 1s - loss: 0.1340 - MSE: 0.1340 - RMSE: 0.3660 - MAE: 0.0539 - val_loss: 0.0905 - val_MSE: 0.0905 - val_RMSE: 0.3009 - val_MAE: 0.0461\n",
      "Epoch 4/30\n",
      "276/276 - 1s - loss: 0.0809 - MSE: 0.0809 - RMSE: 0.2844 - MAE: 0.0432 - val_loss: 0.0476 - val_MSE: 0.0476 - val_RMSE: 0.2181 - val_MAE: 0.0365\n",
      "Epoch 5/30\n",
      "276/276 - 1s - loss: 0.0646 - MSE: 0.0646 - RMSE: 0.2541 - MAE: 0.0376 - val_loss: 0.0460 - val_MSE: 0.0460 - val_RMSE: 0.2145 - val_MAE: 0.0314\n",
      "Epoch 6/30\n",
      "276/276 - 1s - loss: 0.0529 - MSE: 0.0529 - RMSE: 0.2300 - MAE: 0.0337 - val_loss: 0.0250 - val_MSE: 0.0250 - val_RMSE: 0.1582 - val_MAE: 0.0284\n",
      "Epoch 7/30\n",
      "276/276 - 1s - loss: 0.0361 - MSE: 0.0361 - RMSE: 0.1900 - MAE: 0.0290 - val_loss: 0.0199 - val_MSE: 0.0199 - val_RMSE: 0.1410 - val_MAE: 0.0251\n",
      "Epoch 8/30\n",
      "276/276 - 1s - loss: 0.0312 - MSE: 0.0312 - RMSE: 0.1766 - MAE: 0.0258 - val_loss: 0.0175 - val_MSE: 0.0175 - val_RMSE: 0.1324 - val_MAE: 0.0238\n",
      "Epoch 9/30\n",
      "276/276 - 1s - loss: 0.0397 - MSE: 0.0397 - RMSE: 0.1992 - MAE: 0.0281 - val_loss: 0.0212 - val_MSE: 0.0212 - val_RMSE: 0.1456 - val_MAE: 0.0246\n",
      "Epoch 10/30\n",
      "276/276 - 1s - loss: 0.0424 - MSE: 0.0424 - RMSE: 0.2058 - MAE: 0.0271 - val_loss: 0.0169 - val_MSE: 0.0169 - val_RMSE: 0.1300 - val_MAE: 0.0218\n",
      "Epoch 11/30\n",
      "276/276 - 1s - loss: 0.0282 - MSE: 0.0282 - RMSE: 0.1678 - MAE: 0.0234 - val_loss: 0.0142 - val_MSE: 0.0142 - val_RMSE: 0.1191 - val_MAE: 0.0220\n",
      "Epoch 12/30\n",
      "276/276 - 1s - loss: 0.0247 - MSE: 0.0247 - RMSE: 0.1572 - MAE: 0.0233 - val_loss: 0.0459 - val_MSE: 0.0459 - val_RMSE: 0.2142 - val_MAE: 0.0270\n",
      "Epoch 13/30\n",
      "276/276 - 1s - loss: 0.0314 - MSE: 0.0314 - RMSE: 0.1773 - MAE: 0.0261 - val_loss: 0.0119 - val_MSE: 0.0119 - val_RMSE: 0.1089 - val_MAE: 0.0234\n",
      "Epoch 14/30\n",
      "276/276 - 1s - loss: 0.0222 - MSE: 0.0222 - RMSE: 0.1490 - MAE: 0.0237 - val_loss: 0.0159 - val_MSE: 0.0159 - val_RMSE: 0.1261 - val_MAE: 0.0203\n",
      "Epoch 15/30\n",
      "276/276 - 1s - loss: 0.0192 - MSE: 0.0192 - RMSE: 0.1386 - MAE: 0.0216 - val_loss: 0.0139 - val_MSE: 0.0139 - val_RMSE: 0.1178 - val_MAE: 0.0239\n",
      "Epoch 16/30\n",
      "276/276 - 1s - loss: 0.0227 - MSE: 0.0227 - RMSE: 0.1507 - MAE: 0.0231 - val_loss: 0.0141 - val_MSE: 0.0141 - val_RMSE: 0.1188 - val_MAE: 0.0189\n",
      "Epoch 17/30\n",
      "276/276 - 1s - loss: 0.0175 - MSE: 0.0175 - RMSE: 0.1324 - MAE: 0.0202 - val_loss: 0.0096 - val_MSE: 0.0096 - val_RMSE: 0.0981 - val_MAE: 0.0264\n",
      "Epoch 18/30\n",
      "276/276 - 1s - loss: 0.0338 - MSE: 0.0338 - RMSE: 0.1838 - MAE: 0.0259 - val_loss: 0.0293 - val_MSE: 0.0293 - val_RMSE: 0.1711 - val_MAE: 0.0353\n",
      "Epoch 19/30\n",
      "276/276 - 1s - loss: 0.0236 - MSE: 0.0236 - RMSE: 0.1536 - MAE: 0.0236 - val_loss: 0.0111 - val_MSE: 0.0111 - val_RMSE: 0.1054 - val_MAE: 0.0163\n",
      "Epoch 20/30\n",
      "276/276 - 1s - loss: 0.0242 - MSE: 0.0242 - RMSE: 0.1554 - MAE: 0.0246 - val_loss: 0.0173 - val_MSE: 0.0173 - val_RMSE: 0.1316 - val_MAE: 0.0192\n",
      "Epoch 21/30\n",
      "276/276 - 1s - loss: 0.0203 - MSE: 0.0203 - RMSE: 0.1425 - MAE: 0.0205 - val_loss: 0.0075 - val_MSE: 0.0075 - val_RMSE: 0.0866 - val_MAE: 0.0187\n",
      "Epoch 22/30\n",
      "276/276 - 1s - loss: 0.0141 - MSE: 0.0141 - RMSE: 0.1188 - MAE: 0.0190 - val_loss: 0.0086 - val_MSE: 0.0086 - val_RMSE: 0.0928 - val_MAE: 0.0191\n",
      "Epoch 23/30\n",
      "276/276 - 1s - loss: 0.0146 - MSE: 0.0146 - RMSE: 0.1207 - MAE: 0.0201 - val_loss: 0.0091 - val_MSE: 0.0091 - val_RMSE: 0.0955 - val_MAE: 0.0262\n",
      "Epoch 24/30\n",
      "276/276 - 1s - loss: 0.0123 - MSE: 0.0123 - RMSE: 0.1108 - MAE: 0.0190 - val_loss: 0.0077 - val_MSE: 0.0077 - val_RMSE: 0.0876 - val_MAE: 0.0179\n",
      "Epoch 25/30\n",
      "276/276 - 1s - loss: 0.0151 - MSE: 0.0151 - RMSE: 0.1230 - MAE: 0.0199 - val_loss: 0.0131 - val_MSE: 0.0131 - val_RMSE: 0.1144 - val_MAE: 0.0181\n",
      "Epoch 26/30\n",
      "276/276 - 1s - loss: 0.0182 - MSE: 0.0182 - RMSE: 0.1349 - MAE: 0.0232 - val_loss: 0.0100 - val_MSE: 0.0100 - val_RMSE: 0.0998 - val_MAE: 0.0191\n",
      "Epoch 27/30\n",
      "276/276 - 1s - loss: 0.0301 - MSE: 0.0301 - RMSE: 0.1734 - MAE: 0.0249 - val_loss: 0.0094 - val_MSE: 0.0094 - val_RMSE: 0.0968 - val_MAE: 0.0165\n",
      "Epoch 28/30\n",
      "276/276 - 1s - loss: 0.0154 - MSE: 0.0154 - RMSE: 0.1242 - MAE: 0.0179 - val_loss: 0.0101 - val_MSE: 0.0101 - val_RMSE: 0.1003 - val_MAE: 0.0450\n",
      "Epoch 29/30\n",
      "276/276 - 1s - loss: 0.0143 - MSE: 0.0143 - RMSE: 0.1196 - MAE: 0.0210 - val_loss: 0.0226 - val_MSE: 0.0226 - val_RMSE: 0.1504 - val_MAE: 0.0223\n",
      "Epoch 30/30\n",
      "276/276 - 1s - loss: 0.0132 - MSE: 0.0132 - RMSE: 0.1151 - MAE: 0.0194 - val_loss: 0.0078 - val_MSE: 0.0078 - val_RMSE: 0.0883 - val_MAE: 0.0146\n",
      "36.749733686447144 s to train model.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Fit the network\n",
    "###############################################################################\n",
    "print ('\\nFitting the network.')\n",
    "startTime = time.time ()\n",
    "history = clf.fit (X_train, X_train,\n",
    "                         batch_size = BATCH_SIZE,\n",
    "                         epochs = NUMBER_OF_EPOCHS,\n",
    "                         verbose = 2, #1 = progress bar, not useful for logging\n",
    "                         workers = 0,\n",
    "                         use_multiprocessing = True,\n",
    "                         #class_weight = 'auto',\n",
    "                         validation_data = (X_val, X_val))\n",
    "print (str (time.time () - startTime), 's to train model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Analyze results\n",
    "###############################################################################\n",
    "X_val_pred   = clf.predict (X_val)\n",
    "X_train_pred = clf.predict (X_train)\n",
    "print ('Train error:'     , mean_squared_error (X_train_pred, X_train))\n",
    "print ('Validation error:', mean_squared_error (X_val_pred, X_val))\n",
    "\n",
    "#SAMPLES = 50\n",
    "#print ('Error on first', SAMPLES, 'samples:')\n",
    "#print ('MSE (pred, real)')\n",
    "#for pred_sample, real_sample in zip (X_val_pred [:SAMPLES], X_val [:SAMPLES]):\n",
    "#  print (mean_squared_error (pred_sample, real_sample))\n",
    "\n",
    "### K: This looks like another hyperparameter to be adjusted by using a\n",
    "### separate validation set that contains normal and anomaly samples.\n",
    "### K: I've guessed 1%, this may be a future line of research.\n",
    "THRESHOLD_SAMPLE_PERCENTAGE = 1/100\n",
    "\n",
    "train_mse_element_wise = np.mean (np.square (X_train_pred - X_train), axis = 1)\n",
    "val_mse_element_wise = np.mean (np.square (X_val_pred - X_val), axis = 1)\n",
    "\n",
    "max_threshold_val = np.max (val_mse_element_wise)\n",
    "print ('max_Thresh val:', max_threshold_val)\n",
    "\n",
    "\n",
    "\n",
    "print ('samples:')\n",
    "print (int (round (val_mse_element_wise.shape [0] *\n",
    "           THRESHOLD_SAMPLE_PERCENTAGE)))\n",
    "\n",
    "top_n_values_val = np.partition (-val_mse_element_wise,\n",
    "                                 int (round (val_mse_element_wise.shape [0] *\n",
    "                                             THRESHOLD_SAMPLE_PERCENTAGE)))\n",
    "\n",
    "top_n_values_val = -top_n_values_val [: int (round (val_mse_element_wise.shape [0] *\n",
    "                                                    THRESHOLD_SAMPLE_PERCENTAGE))]\n",
    "\n",
    "\n",
    "### K: O limiar de classificacao sera a mediana dos N maiores custos obtidos\n",
    "### ao validar a rede no conjunto de validacao. N e um hiperparametro que pode\n",
    "### ser ajustado, mas e necessario um conjunto de validacao com amostras\n",
    "### anomalas em adicao ao conjunto de validacao atual, que so tem amostras nao\n",
    "### anomalas. @TODO: Desenvolver e validar o conjunto com esta nova tecnica.\n",
    "threshold = np.median (top_n_values_val)\n",
    "print ('Thresh val:', threshold)\n",
    "\n",
    "\n",
    "### K: NOTE: Only look at test results when publishing...\n",
    "sys.exit ()\n",
    "X_test_pred = clf.predict (X_test)\n",
    "print (X_test_pred.shape)\n",
    "print ('Test error:', mean_squared_error (X_test_pred, X_test))\n",
    "\n",
    "\n",
    "y_pred = np.mean (np.square (X_test_pred - X_test), axis = 1)\n",
    "#y_pred = []\n",
    "#for pred_sample, real_sample, label in zip (X_test_pred, X_test, y_test):\n",
    "#  y_pred.append (mean_squared_error (pred_sample, real_sample))\n",
    "\n",
    "#print ('\\nLabel | MSE (pred, real)')\n",
    "#for label, pred in zip (y_test, y_pred):\n",
    "#  print (label, '|', pred)\n",
    "\n",
    "y_test, y_pred = zip (*sorted (zip (y_test, y_pred)))\n",
    "#print ('\\nLabel | MSE (pred, real) (ordered)')\n",
    "#for label, pred in zip (y_test, y_pred):\n",
    "#  print (label, '|', pred)\n",
    "\n",
    "# 0 == normal\n",
    "# 1 == attack\n",
    "print ('\\nPerformance on TEST set:')\n",
    "print ('\\nMSE (pred, real) | Label (ordered)')\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for label, pred in zip (y_test, y_pred):\n",
    "#  if (pred >= threshold):\n",
    "#    print ('Classified as anomaly     (NORMAL):', label)\n",
    "#  else:\n",
    "#    print ('Classified as not anomaly (ATTACK):', label)\n",
    "\n",
    "  if ((pred >= threshold) and (label == 0)):\n",
    "    print ('True negative.')\n",
    "    tn += 1\n",
    "  elif ((pred >= threshold) and (label == 1)):\n",
    "    print ('False negative!')\n",
    "    fn += 1\n",
    "  elif ((pred < threshold) and (label == 1)):\n",
    "    print ('True positive.')\n",
    "    tp += 1\n",
    "  elif ((pred < threshold) and (label == 0)):\n",
    "    print ('False positive!')\n",
    "    fp += 1\n",
    "\n",
    "print ('Confusion matrix:')\n",
    "print ('tp | fp')\n",
    "print ('fn | tn')\n",
    "print (tp, '|', fp)\n",
    "print (fn, '|', tn)\n",
    "print ('TP:', tp)\n",
    "print ('TN:', tn)\n",
    "print ('FP:', fp)\n",
    "print ('FN:', fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
