{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kaylani Bochie\n",
    "# github.com/kaylani2\n",
    "# kaylani AT gta DOT ufrj DOT br\n",
    "\n",
    "### K: Model: SOM\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "sys.path.insert(1, '../')\n",
    "import numpy as np\n",
    "from numpy import mean, std\n",
    "from matplotlib import pyplot as plt\n",
    "from unit import remove_columns_with_one_value, remove_nan_columns, load_dataset\n",
    "from unit import display_general_information, display_feature_distribution\n",
    "from collections import Counter\n",
    "#from imblearn.over_sampling import RandomOverSampler, RandomUnderSampler\n",
    "import sklearn\n",
    "from sklearn import set_config\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit, RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif, chi2, mutual_info_classif\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import keras.utils\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.constraints import maxnorm\n",
    "import susi\n",
    "from susi.SOMPlots import plot_umatrix, plot_estimation_map\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Define constants\n",
    "###############################################################################\n",
    "pd.set_option ('display.max_rows', None)\n",
    "pd.set_option ('display.max_columns', 5)\n",
    "BOT_IOT_DIRECTORY = '../../../../../datasets/bot-iot/'\n",
    "BOT_IOT_FEATURE_NAMES = 'UNSW_2018_IoT_Botnet_Dataset_Feature_Names.csv'\n",
    "BOT_IOT_FILE_5_PERCENT_SCHEMA = 'UNSW_2018_IoT_Botnet_Full5pc_{}.csv' # 1 - 4\n",
    "FIVE_PERCENT_FILES = 4\n",
    "BOT_IOT_FILE_FULL_SCHEMA = 'UNSW_2018_IoT_Botnet_Dataset_{}.csv' # 1 - 74\n",
    "FULL_FILES = 74\n",
    "FILE_NAME = BOT_IOT_DIRECTORY + BOT_IOT_FILE_5_PERCENT_SCHEMA\n",
    "FEATURES = BOT_IOT_DIRECTORY + BOT_IOT_FEATURE_NAMES\n",
    "NAN_VALUES = ['?', '.']\n",
    "TARGET = 'attack'\n",
    "INDEX_COLUMN = 'pkSeqID'\n",
    "LABELS = ['attack', 'category', 'subcategory']\n",
    "STATE = 0\n",
    "try:\n",
    "  STATE = int (sys.argv [1])\n",
    "except:\n",
    "  pass\n",
    "#for STATE in [1, 2, 3, 4, 5]:\n",
    "np.random.seed (STATE)\n",
    "print ('STATE:', STATE)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Load dataset\n",
    "###############################################################################\n",
    "df = load_dataset (FILE_NAME, FIVE_PERCENT_FILES, INDEX_COLUMN, NAN_VALUES)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Clean dataset\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "### Remove columns with only one value\n",
    "df, log = remove_columns_with_one_value (df, verbose = False)\n",
    "print (log)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "### Remove redundant columns, useless columns and unused targets\n",
    "### K: _number columns are numerical representations of other existing columns.\n",
    "### K: category and subcategory are other labels.\n",
    "### K: saddr and daddr may specialize the model to a single network\n",
    "redundant_columns = ['state_number', 'proto_number', 'flgs_number']\n",
    "other_targets = ['category', 'subcategory']\n",
    "misc_columns = ['saddr', 'daddr']\n",
    "print ('Removing redundant columns:', redundant_columns)\n",
    "print ('Removing useless targets:', other_targets)\n",
    "print ('Removing misc columns:', misc_columns)\n",
    "columns_to_remove = redundant_columns + other_targets + misc_columns\n",
    "df.drop (axis = 'columns', columns = columns_to_remove, inplace = True)\n",
    "\n",
    "###############################################################################\n",
    "### Remove NaN columns (with a lot of NaN values)\n",
    "df, log = remove_nan_columns (df, 1/2, verbose = False)\n",
    "print (log)\n",
    "\n",
    "###############################################################################\n",
    "### Encode categorical features\n",
    "print ('Encoding categorical features (ordinal encoding).')\n",
    "my_encoder = OrdinalEncoder ()\n",
    "df ['flgs'] = my_encoder.fit_transform (df ['flgs'].values.reshape (-1, 1))\n",
    "df ['proto'] = my_encoder.fit_transform (df ['proto'].values.reshape (-1, 1))\n",
    "df ['sport'] = my_encoder.fit_transform (df ['sport'].astype (str).values.reshape (-1, 1))\n",
    "df ['dport'] = my_encoder.fit_transform (df ['dport'].astype (str).values.reshape (-1, 1))\n",
    "df ['state'] = my_encoder.fit_transform (df ['state'].values.reshape (-1, 1))\n",
    "print ('Objects:', list (df.select_dtypes ( ['object']).columns))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Quick sanity check\n",
    "###############################################################################\n",
    "display_general_information (df)\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Split dataset into train and test sets\n",
    "###############################################################################\n",
    "### K: Dataset is too big? Drop.\n",
    "# drop_indices = np.random.choice (df.index, int (df.shape [0] * 0.5),\n",
    "#                                  replace = False)\n",
    "# df = df.drop (drop_indices)\n",
    "TEST_SIZE = 3/10\n",
    "VALIDATION_SIZE = 1/4\n",
    "print ('Splitting dataset (test/train):', TEST_SIZE)\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split (\n",
    "                                               df.loc [:, df.columns != TARGET],\n",
    "                                               df [TARGET],\n",
    "                                               test_size = TEST_SIZE,\n",
    "                                               random_state = STATE,)\n",
    "print ('Splitting dataset (validation/train):', VALIDATION_SIZE)\n",
    "X_train_df, X_val_df, y_train_df, y_val_df = train_test_split (\n",
    "                                             X_train_df,\n",
    "                                             y_train_df,\n",
    "                                             test_size = VALIDATION_SIZE,\n",
    "                                             random_state = STATE,)\n",
    "print ('X_train_df shape:', X_train_df.shape)\n",
    "print ('y_train_df shape:', y_train_df.shape)\n",
    "print ('X_val_df shape:', X_val_df.shape)\n",
    "print ('y_val_df shape:', y_val_df.shape)\n",
    "print ('X_test_df shape:', X_test_df.shape)\n",
    "print ('y_test_df shape:', y_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Convert dataframe to a numpy array\n",
    "###############################################################################\n",
    "print ('\\nConverting dataframe to numpy array.')\n",
    "X_train = X_train_df.values\n",
    "y_train = y_train_df.values\n",
    "X_val = X_val_df.values\n",
    "y_val = y_val_df.values\n",
    "X_test = X_test_df.values\n",
    "y_test = y_test_df.values\n",
    "print ('X_train shape:', X_train.shape)\n",
    "print ('y_train shape:', y_train.shape)\n",
    "print ('X_val shape:', X_val.shape)\n",
    "print ('y_val shape:', y_val.shape)\n",
    "print ('X_test shape:', X_test.shape)\n",
    "print ('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for column in df.columns:\n",
    "#    hist = df [column].hist(bins=3)\n",
    "#    print (column)\n",
    "#    plt.scatter(df [column], df ['attack'], s=100,)# c=df.female\n",
    "#    plt.show ()\n",
    "\n",
    "\n",
    "### PLOT\n",
    "print (type (X_train))\n",
    "#print (((X_train_df.dtypes)))\n",
    "#print (df ['spkts'].values)\n",
    "\n",
    "#hist = df.hist (column = 'mean', figsize = (12, 8))\n",
    "\n",
    "### K: .count () takes a bit too long...\n",
    "#df.groupby ('mean').count ().plot ()\n",
    "\n",
    "\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "\n",
    "for column in df.columns:\n",
    "    \n",
    "    print (column)\n",
    "    ax = df.hist(column=column,# bins = np.logspace(np.log10(0.1),np.log10(1.0), 50),\n",
    "                 bins = 40,\n",
    "                 #bins = max (df [column].nunique (), 50),\n",
    "                 grid=False, figsize=(12,8), color='#86bf91', zorder=2, rwidth=0.9)\n",
    "    ax = ax[0]\n",
    "    for x in ax:\n",
    "        # Despine\n",
    "#         x.spines['right'].set_visible(False)\n",
    "#         x.spines['top'].set_visible(False)\n",
    "#         x.spines['left'].set_visible(False)\n",
    "\n",
    "        # Switch off ticks\n",
    "        x.tick_params(axis=\"both\", which=\"both\", bottom=\"on\", top=\"on\", labelbottom=\"on\", left=\"on\", right=\"on\", labelleft=\"on\")\n",
    "\n",
    "        # Draw horizontal axis lines\n",
    "        vals = x.get_yticks()\n",
    "        for tick in vals:\n",
    "            x.axhline(y=tick, linestyle='dashed', alpha=0.4, color='#eeeeee', zorder=1)\n",
    "\n",
    "        # Remove title\n",
    "        x.set_title(\"\")\n",
    "\n",
    "        # Set x-axis label\n",
    "        x.set_xlabel(column, labelpad=20, weight='bold', size=22)\n",
    "\n",
    "        # Set y-axis label\n",
    "        x.set_ylabel('# de amostras', labelpad=20, weight='bold', size=22)\n",
    "        \n",
    "        #x.set_xticklabels(x_ticks, rotation=0, fontsize=20)\n",
    "        plt.rc('xtick',labelsize=20)\n",
    "        plt.rc('ytick',labelsize=20)\n",
    "\n",
    "        #x.set_xticklabels(x_ticks, rotation=0, fontsize=20)\n",
    "\n",
    "\n",
    "        # Format y-axis label\n",
    "        x.yaxis.set_major_formatter(StrMethodFormatter('{x:,g}'))\n",
    "        x.figure.savefig (column + '.png')\n",
    "        x.plot ()\n",
    "        \n",
    "\n",
    "\n",
    "#plt.scatter(df ['bytes'], df ['attack'], s=100,)# c=df.female\n",
    "#plt.show ()\n",
    "#plt.scatter ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print pearson\n",
    "import seaborn as sb\n",
    "pearsoncorr = df.corr(method='pearson')\n",
    "print (pearsoncorr)\n",
    "\n",
    "def heatmap(x, y, size):\n",
    "    fig, ax = plt.subplots()\n",
    "    \n",
    "    # Mapping from column names to integer coordinates\n",
    "    x_labels = [v for v in sorted(x.unique())]\n",
    "    y_labels = [v for v in sorted(y.unique())]\n",
    "    x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} \n",
    "    y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} \n",
    "    \n",
    "    size_scale = 500\n",
    "    ax.scatter(\n",
    "        x=x.map(x_to_num), # Use mapping for x\n",
    "        y=y.map(y_to_num), # Use mapping for y\n",
    "        s=size * size_scale, # Vector of square sizes, proportional to size parameter\n",
    "        marker='s' # Use square as scatterplot marker\n",
    "    )\n",
    "    \n",
    "    # Show column labels on the axes\n",
    "    ax.set_xticks([x_to_num[v] for v in x_labels])\n",
    "    ax.set_xticklabels(x_labels, rotation=45, horizontalalignment='right')\n",
    "    ax.set_yticks([y_to_num[v] for v in y_labels])\n",
    "    ax.set_yticklabels(y_labels)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10,10))         # Sample figsize in inches\n",
    "#sns.heatmap(df1.iloc[:, 1:6:], annot=True, linewidths=.5, ax=ax)\n",
    "sb_plot = sb.heatmap(pearsoncorr, \n",
    "            xticklabels=pearsoncorr.columns,\n",
    "            yticklabels=pearsoncorr.columns,\n",
    "            cmap='RdBu_r',\n",
    "            #annot=True,\n",
    "            linewidth=0.5)\n",
    "sb_plot.figure.savefig(\"output.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot scatter\n",
    "#fig, ax = plt.subplots(figsize=(12,12))         # Sample figsize in inches\n",
    "\n",
    "\n",
    "#pd_plot = pd.plotting.scatter_matrix(df.sample(frac=0.01, random_state=STATE), alpha=0.2)\n",
    "#pd_plot.figure.savefig(\"output2.png\")\n",
    "print (len (X_train [:, 0]))\n",
    "print (len (y_train))\n",
    "\n",
    "\n",
    "#plt.scatter(X_train [:, 0], y_train)\n",
    "#for i in range (len (X_train [0])):\n",
    "for i in range (5):\n",
    "    #plt.scatter(X_train [:, i], y_train)\n",
    "    plt.scatter(X_train [:, i], X_train [:, 2])\n",
    "\n",
    "    plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot histograms\n",
    "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "'''\n",
    "n, bins, patches = plt.hist(x=X_train [:, 0], bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('My Very Own Histogram')\n",
    "#plt.text(23, 45, r'$\\mu=15, b=3$')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)\n",
    "plt.show ()\n",
    "''' \n",
    "\n",
    "for column in df.columns:\n",
    "    df [column].plot.hist(grid=True, bins=20, rwidth=0.9,\n",
    "                       color='#607c8e')\n",
    "    plt.title(column)\n",
    "    plt.xlabel('Ocorrências')\n",
    "    plt.ylabel(column)\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Apply normalization\n",
    "###############################################################################\n",
    "### K: NOTE: Only use derived information from the train set to avoid leakage.\n",
    "print ('\\nApplying normalization.')\n",
    "startTime = time.time ()\n",
    "scaler = StandardScaler ()\n",
    "#scaler = MinMaxScaler (feature_range = (0, 1))\n",
    "scaler.fit (X_train)\n",
    "X_train = scaler.transform (X_train)\n",
    "X_val = scaler.transform (X_val)\n",
    "X_test = scaler.transform (X_test)\n",
    "print (str (time.time () - startTime), 'to normalize data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Perform feature selection\n",
    "###############################################################################\n",
    "NUMBER_OF_FEATURES = 9 #X_train.shape [-1]\n",
    "print ('\\nSelecting top', NUMBER_OF_FEATURES, 'features.')\n",
    "fs = PCA (n_components = NUMBER_OF_FEATURES)\n",
    "fs.fit (X_train)\n",
    "print (fs.explained_variance_ratio_)\n",
    "print (fs.singular_values_)\n",
    "X_train = fs.transform (X_train)\n",
    "X_val = fs.transform (X_val)\n",
    "X_test = fs.transform (X_test)\n",
    "print ('X_train shape:', X_train.shape)\n",
    "print ('y_train shape:', y_train.shape)\n",
    "print ('X_val shape:', X_val.shape)\n",
    "print ('y_val shape:', y_val.shape)\n",
    "print ('X_test shape:', X_test.shape)\n",
    "print ('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "som = susi.SOMClustering(\n",
    "    n_rows=17,\n",
    "    n_columns=17,\n",
    "    verbose = 2,\n",
    "    n_jobs = 4,\n",
    ")\n",
    "startTime = time.time ()\n",
    "som.fit(X_train)\n",
    "print (str (time.time () - startTime), 's to train model.')\n",
    "print(\"SOM fitted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print (som.score (X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_matrix = som.get_u_matrix()\n",
    "plot_umatrix(u_matrix, 17, 17)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = som.get_clusters(X_train)\n",
    "plt.scatter(x=[c[1] for c in clusters], y=[c[0] for c in clusters], c=y, alpha=0.2)\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
