{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Author: Kaylani Bochie\n",
    "# github.com/kaylani2\n",
    "# kaylani AT gta DOT ufrj DOT br\n",
    "\n",
    "### K: Model: Autoencoder\n",
    "import sys\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import math\n",
    "import seaborn as sb\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.ticker import StrMethodFormatter\n",
    "sys.path.insert(1, '../')\n",
    "from numpy import mean, std\n",
    "from unit import remove_columns_with_one_value, remove_nan_columns, load_dataset\n",
    "from unit import display_general_information, display_feature_distribution\n",
    "from collections import Counter\n",
    "#from imblearn.over_sampling import RandomOverSampler, RandomUnderSampler\n",
    "import sklearn\n",
    "from sklearn import set_config\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score\n",
    "from sklearn.metrics import f1_score, classification_report, accuracy_score\n",
    "from sklearn.metrics import cohen_kappa_score, mean_squared_error\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split, PredefinedSplit\n",
    "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif, chi2, mutual_info_classif\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tensorflow.keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "import keras.utils\n",
    "from keras import metrics\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM\n",
    "from keras.optimizers import RMSprop, Adam\n",
    "from keras.constraints import maxnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STATE: 0\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Define constants\n",
    "###############################################################################\n",
    "pd.set_option ('display.max_rows', None)\n",
    "pd.set_option ('display.max_columns', 5)\n",
    "BOT_IOT_DIRECTORY = '../../../../../datasets/bot-iot/'\n",
    "BOT_IOT_FEATURE_NAMES = 'UNSW_2018_IoT_Botnet_Dataset_Feature_Names.csv'\n",
    "BOT_IOT_FILE_5_PERCENT_SCHEMA = 'UNSW_2018_IoT_Botnet_Full5pc_{}.csv' # 1 - 4\n",
    "FIVE_PERCENT_FILES = 4\n",
    "BOT_IOT_FILE_FULL_SCHEMA = 'UNSW_2018_IoT_Botnet_Dataset_{}.csv' # 1 - 74\n",
    "FULL_FILES = 74\n",
    "FILE_NAME = BOT_IOT_DIRECTORY + BOT_IOT_FILE_5_PERCENT_SCHEMA\n",
    "FEATURES = BOT_IOT_DIRECTORY + BOT_IOT_FEATURE_NAMES\n",
    "NAN_VALUES = ['?', '.']\n",
    "TARGET = 'attack'\n",
    "INDEX_COLUMN = 'pkSeqID'\n",
    "LABELS = ['attack', 'category', 'subcategory']\n",
    "STATE = 0\n",
    "try:\n",
    "  STATE = int (sys.argv [1])\n",
    "except:\n",
    "  pass\n",
    "#for STATE in [1, 2, 3, 4, 5]:\n",
    "np.random.seed (STATE)\n",
    "print ('STATE:', STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ../../../../../datasets/bot-iot/UNSW_2018_IoT_Botnet_Full5pc_1.csv\n",
      "Reading ../../../../../datasets/bot-iot/UNSW_2018_IoT_Botnet_Full5pc_2.csv\n",
      "Reading ../../../../../datasets/bot-iot/UNSW_2018_IoT_Botnet_Full5pc_3.csv\n",
      "Reading ../../../../../datasets/bot-iot/UNSW_2018_IoT_Botnet_Full5pc_4.csv\n",
      "Finished loading dataset.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Load dataset\n",
    "###############################################################################\n",
    "df = load_dataset (FILE_NAME, FIVE_PERCENT_FILES, INDEX_COLUMN, NAN_VALUES)\n",
    "print ('Finished loading dataset.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Quick sanity check\n",
    "###############################################################################M\n",
    "display_general_information (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "While removing single value columns: No columns dropped.\n",
      "Removing redundant columns: ['state_number', 'proto_number', 'flgs_number']\n",
      "Removing useless targets: ['category', 'subcategory']\n",
      "Removing misc columns: ['saddr', 'daddr']\n",
      "While removing nan value columns: No columns dropped.\n",
      "Encoding categorical features (ordinal encoding).\n",
      "Objects: [] (should be empty)\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Clean dataset\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "### Remove columns with only one value\n",
    "df, log = remove_columns_with_one_value (df, verbose = False)\n",
    "print (log)\n",
    "\n",
    "###############################################################################\n",
    "### Remove redundant columns, useless columns and unused targets\n",
    "### K: _number columns are numerical representations of other existing columns.\n",
    "### K: category and subcategory are other labels.\n",
    "### K: saddr and daddr may specialize the model to a single network\n",
    "redundant_columns = ['state_number', 'proto_number', 'flgs_number']\n",
    "other_targets = ['category', 'subcategory']\n",
    "misc_columns = ['saddr', 'daddr']\n",
    "print ('Removing redundant columns:', redundant_columns)\n",
    "print ('Removing useless targets:', other_targets)\n",
    "print ('Removing misc columns:', misc_columns)\n",
    "columns_to_remove = redundant_columns + other_targets + misc_columns\n",
    "df.drop (axis = 'columns', columns = columns_to_remove, inplace = True)\n",
    "\n",
    "###############################################################################\n",
    "### Remove NaN columns (with a lot of NaN values)\n",
    "df, log = remove_nan_columns (df, 1/2, verbose = False)\n",
    "print (log)\n",
    "\n",
    "###############################################################################\n",
    "### Encode categorical features\n",
    "print ('Encoding categorical features (ordinal encoding).')\n",
    "my_encoder = OrdinalEncoder ()\n",
    "df ['flgs'] = my_encoder.fit_transform (df ['flgs'].values.reshape (-1, 1))\n",
    "df ['proto'] = my_encoder.fit_transform (df ['proto'].values.reshape (-1, 1))\n",
    "df ['sport'] = my_encoder.fit_transform (df ['sport'].astype (str).values.reshape (-1, 1))\n",
    "df ['dport'] = my_encoder.fit_transform (df ['dport'].astype (str).values.reshape (-1, 1))\n",
    "df ['state'] = my_encoder.fit_transform (df ['state'].values.reshape (-1, 1))\n",
    "print ('Objects:', list (df.select_dtypes ( ['object']).columns), '(should be empty)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Quick sanity check\n",
    "###############################################################################M\n",
    "display_general_information (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot correlation matrix\n",
    "pearsoncorr = df.corr (method = 'pearson')\n",
    "\n",
    "fig, ax = plt.subplots (figsize = (10, 10))  # Sample figsize in inches\n",
    "sb_plot = sb.heatmap (pearsoncorr, \n",
    "                      xticklabels = pearsoncorr.columns,\n",
    "                      yticklabels = pearsoncorr.columns,\n",
    "                      cmap = 'RdBu_r',\n",
    "                      #annot=True,\n",
    "                      linewidth = 0.5)\n",
    "sb_plot.figure.savefig ('correlation_matrix.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot feature distribution\n",
    "### K: .count () takes a bit too long...\n",
    "#df.groupby ('mean').count ().plot ()\n",
    "\n",
    "for column in df.columns:   \n",
    "    #print (column)\n",
    "    ax = df.hist (column=column,# bins = np.logspace(np.log10(0.1),np.log10(1.0), 50),\n",
    "                  bins = 40,\n",
    "                  #bins = max (df [column].nunique (), 50),\n",
    "                  grid=False, figsize=(12,8), color='#86bf91', zorder=2, rwidth=0.9)\n",
    "    ax = ax[0]\n",
    "    for x in ax:\n",
    "        # Switch off ticks\n",
    "        x.tick_params(axis=\"both\", which=\"both\", bottom=\"on\", top=\"on\", labelbottom=\"on\", left=\"on\", right=\"on\", labelleft=\"on\")\n",
    "        # Draw horizontal axis lines\n",
    "        vals = x.get_yticks()\n",
    "        for tick in vals:\n",
    "            x.axhline(y=tick, linestyle='dashed', alpha=0.4, color='#eeeeee', zorder=1)\n",
    "        # Remove title\n",
    "        x.set_title(\"\")\n",
    "        # Set x-axis label\n",
    "        x.set_xlabel(column, labelpad=20, weight='bold', size=22)\n",
    "        # Set y-axis label\n",
    "        x.set_ylabel('Samples', labelpad=20, weight='bold', size=22)\n",
    "        #x.set_xticklabels(x_ticks, rotation=0, fontsize=20)\n",
    "        plt.rc('xtick',labelsize=20)\n",
    "        plt.rc('ytick',labelsize=20)\n",
    "        #x.set_xticklabels(x_ticks, rotation=0, fontsize=20)\n",
    "        # Format y-axis label\n",
    "        x.yaxis.set_major_formatter(StrMethodFormatter('{x:,g}'))\n",
    "        x.figure.savefig (column + '.png')\n",
    "        #x.plot ()\n",
    "print ('Done plotting feature distribution.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot feature distribution\n",
    "# An \"interface\" to matplotlib.axes.Axes.hist() method\n",
    "'''\n",
    "n, bins, patches = plt.hist(x=X_train [:, 0], bins='auto', color='#0504aa',\n",
    "                            alpha=0.7, rwidth=0.85)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('My Very Own Histogram')\n",
    "#plt.text(23, 45, r'$\\mu=15, b=3$')\n",
    "maxfreq = n.max()\n",
    "# Set a clean upper y-axis limit.\n",
    "plt.ylim(ymax=np.ceil(maxfreq / 10) * 10 if maxfreq % 10 else maxfreq + 10)\n",
    "plt.show ()\n",
    "''' \n",
    "\n",
    "for column in df.columns:\n",
    "    df [column].plot.hist(grid=True, bins=20, rwidth=0.9, color='#607c8e')\n",
    "    plt.title(column)\n",
    "    plt.xlabel(column)\n",
    "    plt.ylabel('Samples')\n",
    "    plt.grid(axis='y', alpha=0.75)\n",
    "    plt.show ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Split dataset\n",
    "###############################################################################\n",
    "## K: Dataset is too big? Drop.\n",
    "drop_indices = np.random.choice (df.index, int (df.shape [0] * 0.5),\n",
    "                                 replace = False)\n",
    "df = df.drop (drop_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack set:\n",
      "1    1834017\n",
      "Name: attack, dtype: int64\n",
      "Normal set:\n",
      "0    244\n",
      "Name: attack, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "### Isolate attack and non-attack (normal) samples\n",
    "mask = df [TARGET] == 0\n",
    "# 0 == normal\n",
    "df_normal = df [mask]\n",
    "# 1 == attack\n",
    "df_attack = df [~mask]\n",
    "\n",
    "print ('Attack set:')\n",
    "print (df_attack [TARGET].value_counts ())\n",
    "print ('Normal set:')\n",
    "print (df_normal [TARGET].value_counts ())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set:\n",
      "1    1833773\n",
      "Name: attack, dtype: int64\n",
      "Test set:\n",
      "1    244\n",
      "0    244\n",
      "Name: attack, dtype: int64\n",
      "\n",
      "Splitting dataset (validation/train): 0.25\n"
     ]
    }
   ],
   "source": [
    "### Sample and drop random attacks\n",
    "df_random_attacks = df_attack.sample (n = df_normal.shape [0],\n",
    "                                      random_state = STATE)\n",
    "df_attack = df_attack.drop (df_random_attacks.index)\n",
    "\n",
    "### Assemble train set (only attacks)\n",
    "X_train = df_attack.loc [:, df.columns != TARGET]\n",
    "y_train = df_attack [TARGET]\n",
    "print ('Train set:')\n",
    "print (df_attack [TARGET].value_counts ())\n",
    "\n",
    "### Assemble test set (50/50 attacks and non-attacks)\n",
    "df_test = pd.DataFrame ()\n",
    "df_test = pd.concat ( [df_test, df_normal])\n",
    "df_test = pd.concat ( [df_test, df_random_attacks])\n",
    "print ('Test set:')\n",
    "print (df_test [TARGET].value_counts ())\n",
    "X_test = df_test.loc [:, df.columns != TARGET]\n",
    "y_test = df_test [TARGET]\n",
    "### K: y_test is required to plot the roc curve in the end\n",
    "\n",
    "# df_train = df_attack\n",
    "VALIDATION_SIZE = 1/4\n",
    "print ('\\nSplitting dataset (validation/train):', VALIDATION_SIZE)\n",
    "X_train, X_val, y_train, y_val = train_test_split (\n",
    "                                             X_train,\n",
    "                                             y_train,\n",
    "                                             test_size = VALIDATION_SIZE,\n",
    "                                             random_state = STATE,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('X_train shape:', X_train.shape)\n",
    "print ('y_train shape:', y_train.shape)\n",
    "print ('X_val shape:', X_val.shape)\n",
    "print ('y_val shape:', y_val.shape)\n",
    "print ('X_test shape:', X_test.shape)\n",
    "print ('y_test shape:', y_test.shape)\n",
    "print (type (X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###############################################################################\n",
    "# ## Convert dataframe to a numpy array\n",
    "# ###############################################################################\n",
    "# print ('\\nConverting dataframe to numpy array.')\n",
    "# X_train = X_train.values\n",
    "# y_train = y_train.values\n",
    "# X_val = X_val.values\n",
    "# y_val = y_val.values\n",
    "# X_test = X_test.values\n",
    "# y_test = y_test.values\n",
    "# print ('X_train shape:', X_train.shape)\n",
    "# print ('y_train shape:', y_train.shape)\n",
    "# print ('X_val shape:', X_val.shape)\n",
    "# print ('y_val shape:', y_val.shape)\n",
    "# print ('X_test shape:', X_test.shape)\n",
    "# print ('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Applying normalization.\n",
      "1.284205675125122 to normalize data.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Apply normalization\n",
    "###############################################################################\n",
    "### K: NOTE: Only use derived information from the train set to avoid leakage.\n",
    "print ('\\nApplying normalization.')\n",
    "startTime = time.time ()\n",
    "scaler = StandardScaler ()\n",
    "scaler.fit (X_train)\n",
    "X_train = scaler.transform (X_train)\n",
    "X_val = scaler.transform (X_val)\n",
    "X_test = scaler.transform (X_test)\n",
    "print (str (time.time () - startTime), 'to normalize data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('X_train shape:', X_train.shape)\n",
    "print ('y_train shape:', y_train.shape)\n",
    "print ('X_val shape:', X_val.shape)\n",
    "print ('y_val shape:', y_val.shape)\n",
    "print ('X_test shape:', X_test.shape)\n",
    "print ('y_test shape:', y_test.shape)\n",
    "print (type (X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Selecting top 15 features.\n",
      "[0.29275414 0.1113438  0.09904516 0.06916064 0.05744228 0.04058369\n",
      " 0.0385783  0.03403591 0.03279513 0.02985544 0.02816172 0.02643185\n",
      " 0.02540309 0.02318213 0.01827161]\n",
      "[3859.71897053 2380.33006749 2245.02297231 1876.0035388  1709.70027251\n",
      " 1437.07671729 1401.12113079 1316.05147808 1291.8402985  1232.58215301\n",
      " 1197.10911841 1159.75950711 1136.96583017 1086.12746772  964.25655451]\n",
      "X_train shape: (1375329, 15)\n",
      "y_train shape: (1375329,)\n",
      "X_val shape: (458444, 15)\n",
      "y_val shape: (458444,)\n",
      "X_test shape: (488, 15)\n",
      "y_test shape: (488,)\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Perform feature selection\n",
    "###############################################################################\n",
    "NUMBER_OF_FEATURES = 15 #X_train.shape [-1]\n",
    "print ('\\nSelecting top', NUMBER_OF_FEATURES, 'features. (PCA)')\n",
    "fs = PCA (n_components = NUMBER_OF_FEATURES)\n",
    "fs.fit (X_train)\n",
    "print (fs.explained_variance_ratio_)\n",
    "print (fs.singular_values_)\n",
    "X_train = fs.transform (X_train)\n",
    "X_val = fs.transform (X_val)\n",
    "X_test = fs.transform (X_test)\n",
    "print ('X_train shape:', X_train.shape)\n",
    "print ('y_train shape:', y_train.shape)\n",
    "print ('X_val shape:', X_val.shape)\n",
    "print ('y_val shape:', y_val.shape)\n",
    "print ('X_test shape:', X_test.shape)\n",
    "print ('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "## Create learning model (Autoencoder) and tune hyperparameters\n",
    "###############################################################################\n",
    "###############################################################################\n",
    "# Hyperparameter tuning\n",
    "test_fold = np.repeat ([-1, 0], [X_train.shape [0], X_val.shape [0]])\n",
    "myPreSplit = PredefinedSplit (test_fold)\n",
    "def create_model (learn_rate = 0.01, dropout_rate = 0.0, weight_constraint = 0,\n",
    "                  metrics = ['mse'], input_layer_neurons,\n",
    "                  neurons_on_first_layer = 32,\n",
    "                  second_layer_boolean = False,\n",
    "                  neurons_on_chokehold_layer = 8):\n",
    "  model = Sequential ()\n",
    "  model.add (Dense (input_layer_neurons, activation = 'relu',\n",
    "                   input_shape = (input_layer_neurons, )))\n",
    "  if (second_layer_boolean):\n",
    "    model.add (Dense (neurons_on_first_layer/2, activation = 'relu'))\n",
    "  model.add (Dense (neurons_on_second_layer, activation = 'relu'))\n",
    "  model.add (Dense (neurons_on_chokehold_layer,  activation = 'relu'))\n",
    "  if (second_layer_boolean):\n",
    "    model.add (Dense (neurons_on_first_layer/2, activation = 'relu'))\n",
    "  model.add (Dense (neurons_on_first_layer, activation = 'relu'))\n",
    "  model.add (Dense (input_layer_neurons, activation = None))\n",
    "  model.compile (loss = 'mean_squared_error',\n",
    "                 optimizer = 'adam',\n",
    "                 metrics = metrics)\n",
    "  return model\n",
    "\n",
    "\n",
    "model = KerasRegressor (build_fn = create_model, verbose = 2)\n",
    "input_layer_neurons = X_train.shape [1]\n",
    "batch_size = [5000, 10000]\n",
    "epochs = [200]\n",
    "learn_rate = [0.0001, 0.001]\n",
    "dropout_rate = [0.0]\n",
    "weight_constraint = [0]\n",
    "neurons_on_first_layer = [32, 64]\n",
    "neurons_on_chokehold_layer = [4, 8]\n",
    "second_layer_boolean = [False, True] # Is there another layer?\n",
    "param_grid = dict (batch_size = batch_size, epochs = epochs,\n",
    "                   dropout_rate = dropout_rate, learn_rate = learn_rate,\n",
    "                   weight_constraint = weight_constraint,\n",
    "                   input_layer_neurons = input_layer_neurons,\n",
    "                   neurons_on_first_layer = neurons_on_first_layer,\n",
    "                   second_layer_boolean = second_layer_boolean,\n",
    "                   neurons_on_chokehold_layer = neurons_on_chokehold_layer)\n",
    "grid = GridSearchCV (estimator = model, param_grid = param_grid,\n",
    "                    scoring = 'neg_mean_squared_error', cv = myPreSplit,\n",
    "                    verbose = 2, n_jobs = 1)\n",
    "startTime = time.time ()\n",
    "grid_result = grid.fit (np.vstack ( (X_train, X_val)),#, axis = 1),\n",
    "                       np.vstack ( (X_train, X_val)))#, axis = 1))\n",
    "print (str (time.time () - startTime), 's to search grid.')\n",
    "print (grid_result.best_params_)\n",
    "\n",
    "print (\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "means = grid_result.cv_results_ ['mean_test_score']\n",
    "stds = grid_result.cv_results_ ['std_test_score']\n",
    "params = grid_result.cv_results_ ['params']\n",
    "for mean, stdev, param in zip (means, stds, params):\n",
    "  print (\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "## 9 features:\n",
    "## Best: -0.148847 using {'batch_size': 5000, 'dropout_rate': 0.0, 'epochs': 10, 'learn_rate': 0.001, 'weight_constraint': 0}\n",
    "\n",
    "## All features (SEM PCA): (97s to search grid)\n",
    "## Best: -0.159944 using {'batch_size': 10000, 'dropout_rate': 0.0, 'epochs': 10, 'learn_rate': 0.1, 'weight_constraint': 0}\n",
    "\n",
    "## All features (COM PCA):  (100s to search grid)\n",
    "## Best: -0.100227 using {'batch_size': 5000, 'dropout_rate': 0.0, 'epochs': 10, 'learn_rate': 0.001, 'weight_constraint': 0}\n",
    "\n",
    "## 32 features: (88s to search grid)\n",
    "## Best: -0.179934 using {'batch_size': 5000, 'dropout_rate': 0.0, 'epochs': 10, 'learn_rate': 0.001, 'weight_constraint': 0}\n",
    "\n",
    "## 15 features (67s to search grid)\n",
    "## Best: -0.235642 using {'batch_size': 10000, 'dropout_rate': 0.0, 'epochs': 10, 'learn_rate': 0.1, 'weight_constraint': 0}\n",
    "\n",
    "## 15 features (60min to search grid)\n",
    "## Best: -0.019674 using {'batch_size': 10000, 'dropout_rate': 0.0, 'epochs': 200, 'learn_rate': 0.0001, 'neurons_on_chokehold_layer': 8, 'neurons_on_first_layer': 64, 'weight_constraint': 0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating learning model.\n",
      "\n",
      "Compiling the network.\n",
      "Model summary:\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 15)                240       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                512       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 264       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 15)                495       \n",
      "=================================================================\n",
      "Total params: 1,799\n",
      "Trainable params: 1,799\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Finished model\n",
    "# Best: -0.015576 using {'batch_size': 5000, 'dropout_rate': 0.0, 'epochs': 10,\n",
    "# learn_rate': 0.1, 'weight_constraint': 0}\n",
    "METRICS = [keras.metrics.MeanSquaredError (name = 'MSE'),\n",
    "           keras.metrics.RootMeanSquaredError (name = 'RMSE'),  \n",
    "           keras.metrics.MeanAbsoluteError (name = 'MAE'),]\n",
    "### K: learning rate foi alterado manualmente ao olhar os valores do erro na\n",
    "### validacao ao longo das epochs...\n",
    "NUMBER_OF_EPOCHS = 200\n",
    "BATCH_SIZE = 10000\n",
    "LEARNING_RATE = 0.0001\n",
    "\n",
    "print ('\\nCreating learning model.')\n",
    "clf = Sequential ()\n",
    "clf.add (Dense (X_train.shape [1], activation = 'relu',\n",
    "                      input_shape = (X_train.shape [1], )))\n",
    "clf.add (Dense (32, activation = 'relu'))\n",
    "clf.add (Dense (8,  activation = 'relu'))\n",
    "clf.add (Dense (32, activation = 'relu'))\n",
    "clf.add (Dense (X_train.shape [1], activation = None))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "## Compile the network\n",
    "###############################################################################\n",
    "print ('\\nCompiling the network.')\n",
    "clf.compile (loss = 'mean_squared_error',\n",
    "                   optimizer = Adam (lr = LEARNING_RATE),\n",
    "                   metrics = METRICS)\n",
    "print ('Model summary:')\n",
    "clf.summary ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fitting the network.\n",
      "Epoch 1/200\n",
      "138/138 - 1s - loss: 2.1963 - MSE: 2.1963 - RMSE: 1.4820 - MAE: 0.8263 - val_loss: 2.5231 - val_MSE: 2.5231 - val_RMSE: 1.5884 - val_MAE: 0.8172\n",
      "Epoch 2/200\n",
      "138/138 - 1s - loss: 2.0991 - MSE: 2.0991 - RMSE: 1.4488 - MAE: 0.8121 - val_loss: 2.4175 - val_MSE: 2.4175 - val_RMSE: 1.5548 - val_MAE: 0.8080\n",
      "Epoch 3/200\n",
      "138/138 - 1s - loss: 1.9933 - MSE: 1.9933 - RMSE: 1.4119 - MAE: 0.8015 - val_loss: 2.2934 - val_MSE: 2.2934 - val_RMSE: 1.5144 - val_MAE: 0.7942\n",
      "Epoch 4/200\n",
      "138/138 - 1s - loss: 1.8590 - MSE: 1.8590 - RMSE: 1.3635 - MAE: 0.7834 - val_loss: 2.1301 - val_MSE: 2.1301 - val_RMSE: 1.4595 - val_MAE: 0.7707\n",
      "Epoch 5/200\n",
      "138/138 - 1s - loss: 1.6989 - MSE: 1.6989 - RMSE: 1.3034 - MAE: 0.7553 - val_loss: 1.9576 - val_MSE: 1.9576 - val_RMSE: 1.3991 - val_MAE: 0.7397\n",
      "Epoch 6/200\n",
      "138/138 - 1s - loss: 1.5397 - MSE: 1.5397 - RMSE: 1.2409 - MAE: 0.7212 - val_loss: 1.7983 - val_MSE: 1.7983 - val_RMSE: 1.3410 - val_MAE: 0.7035\n",
      "Epoch 7/200\n",
      "138/138 - 1s - loss: 1.4014 - MSE: 1.4014 - RMSE: 1.1838 - MAE: 0.6856 - val_loss: 1.6736 - val_MSE: 1.6736 - val_RMSE: 1.2937 - val_MAE: 0.6671\n",
      "Epoch 8/200\n",
      "138/138 - 1s - loss: 1.2934 - MSE: 1.2934 - RMSE: 1.1373 - MAE: 0.6507 - val_loss: 1.5767 - val_MSE: 1.5767 - val_RMSE: 1.2557 - val_MAE: 0.6360\n",
      "Epoch 9/200\n",
      "138/138 - 1s - loss: 1.2078 - MSE: 1.2078 - RMSE: 1.0990 - MAE: 0.6232 - val_loss: 1.5019 - val_MSE: 1.5019 - val_RMSE: 1.2255 - val_MAE: 0.6112\n",
      "Epoch 10/200\n",
      "138/138 - 1s - loss: 1.1380 - MSE: 1.1380 - RMSE: 1.0668 - MAE: 0.6000 - val_loss: 1.4366 - val_MSE: 1.4366 - val_RMSE: 1.1986 - val_MAE: 0.5900\n",
      "Epoch 11/200\n",
      "138/138 - 1s - loss: 1.0760 - MSE: 1.0760 - RMSE: 1.0373 - MAE: 0.5814 - val_loss: 1.3793 - val_MSE: 1.3793 - val_RMSE: 1.1745 - val_MAE: 0.5729\n",
      "Epoch 12/200\n",
      "138/138 - 1s - loss: 1.0230 - MSE: 1.0230 - RMSE: 1.0114 - MAE: 0.5635 - val_loss: 1.3219 - val_MSE: 1.3219 - val_RMSE: 1.1497 - val_MAE: 0.5556\n",
      "Epoch 13/200\n",
      "138/138 - 1s - loss: 0.9713 - MSE: 0.9713 - RMSE: 0.9855 - MAE: 0.5470 - val_loss: 1.2728 - val_MSE: 1.2728 - val_RMSE: 1.1282 - val_MAE: 0.5390\n",
      "Epoch 14/200\n",
      "138/138 - 1s - loss: 0.9267 - MSE: 0.9267 - RMSE: 0.9626 - MAE: 0.5301 - val_loss: 1.2267 - val_MSE: 1.2267 - val_RMSE: 1.1076 - val_MAE: 0.5235\n",
      "Epoch 15/200\n",
      "138/138 - 1s - loss: 0.8865 - MSE: 0.8865 - RMSE: 0.9415 - MAE: 0.5164 - val_loss: 1.1838 - val_MSE: 1.1838 - val_RMSE: 1.0880 - val_MAE: 0.5092\n",
      "Epoch 16/200\n",
      "138/138 - 1s - loss: 0.8495 - MSE: 0.8495 - RMSE: 0.9217 - MAE: 0.5013 - val_loss: 1.1425 - val_MSE: 1.1425 - val_RMSE: 1.0689 - val_MAE: 0.4964\n",
      "Epoch 17/200\n",
      "138/138 - 1s - loss: 0.8142 - MSE: 0.8142 - RMSE: 0.9023 - MAE: 0.4896 - val_loss: 1.1029 - val_MSE: 1.1029 - val_RMSE: 1.0502 - val_MAE: 0.4824\n",
      "Epoch 18/200\n",
      "138/138 - 1s - loss: 0.7816 - MSE: 0.7816 - RMSE: 0.8841 - MAE: 0.4765 - val_loss: 1.0661 - val_MSE: 1.0661 - val_RMSE: 1.0325 - val_MAE: 0.4698\n",
      "Epoch 19/200\n",
      "138/138 - 1s - loss: 0.7504 - MSE: 0.7504 - RMSE: 0.8663 - MAE: 0.4636 - val_loss: 1.0255 - val_MSE: 1.0255 - val_RMSE: 1.0127 - val_MAE: 0.4587\n",
      "Epoch 20/200\n",
      "138/138 - 1s - loss: 0.7200 - MSE: 0.7200 - RMSE: 0.8486 - MAE: 0.4517 - val_loss: 0.9892 - val_MSE: 0.9892 - val_RMSE: 0.9946 - val_MAE: 0.4490\n",
      "Epoch 21/200\n",
      "138/138 - 1s - loss: 0.6914 - MSE: 0.6914 - RMSE: 0.8315 - MAE: 0.4429 - val_loss: 0.9538 - val_MSE: 0.9538 - val_RMSE: 0.9766 - val_MAE: 0.4381\n",
      "Epoch 22/200\n",
      "138/138 - 1s - loss: 0.6654 - MSE: 0.6654 - RMSE: 0.8157 - MAE: 0.4331 - val_loss: 0.9181 - val_MSE: 0.9181 - val_RMSE: 0.9582 - val_MAE: 0.4298\n",
      "Epoch 23/200\n",
      "138/138 - 1s - loss: 0.6408 - MSE: 0.6408 - RMSE: 0.8005 - MAE: 0.4246 - val_loss: 0.8873 - val_MSE: 0.8873 - val_RMSE: 0.9420 - val_MAE: 0.4213\n",
      "Epoch 24/200\n",
      "138/138 - 1s - loss: 0.6171 - MSE: 0.6171 - RMSE: 0.7856 - MAE: 0.4179 - val_loss: 0.8545 - val_MSE: 0.8545 - val_RMSE: 0.9244 - val_MAE: 0.4131\n",
      "Epoch 25/200\n",
      "138/138 - 1s - loss: 0.5935 - MSE: 0.5935 - RMSE: 0.7704 - MAE: 0.4085 - val_loss: 0.8220 - val_MSE: 0.8220 - val_RMSE: 0.9066 - val_MAE: 0.4062\n",
      "Epoch 26/200\n",
      "138/138 - 1s - loss: 0.5715 - MSE: 0.5715 - RMSE: 0.7560 - MAE: 0.4011 - val_loss: 0.7986 - val_MSE: 0.7986 - val_RMSE: 0.8936 - val_MAE: 0.3985\n",
      "Epoch 27/200\n",
      "138/138 - 1s - loss: 0.5496 - MSE: 0.5496 - RMSE: 0.7414 - MAE: 0.3948 - val_loss: 0.7605 - val_MSE: 0.7605 - val_RMSE: 0.8720 - val_MAE: 0.3918\n",
      "Epoch 28/200\n",
      "138/138 - 1s - loss: 0.5306 - MSE: 0.5306 - RMSE: 0.7285 - MAE: 0.3886 - val_loss: 0.7368 - val_MSE: 0.7368 - val_RMSE: 0.8584 - val_MAE: 0.3857\n",
      "Epoch 29/200\n",
      "138/138 - 1s - loss: 0.5143 - MSE: 0.5143 - RMSE: 0.7171 - MAE: 0.3824 - val_loss: 0.7124 - val_MSE: 0.7124 - val_RMSE: 0.8440 - val_MAE: 0.3804\n",
      "Epoch 30/200\n",
      "138/138 - 1s - loss: 0.4981 - MSE: 0.4981 - RMSE: 0.7058 - MAE: 0.3779 - val_loss: 0.6896 - val_MSE: 0.6896 - val_RMSE: 0.8304 - val_MAE: 0.3755\n",
      "Epoch 31/200\n",
      "138/138 - 1s - loss: 0.4834 - MSE: 0.4834 - RMSE: 0.6953 - MAE: 0.3727 - val_loss: 0.6697 - val_MSE: 0.6697 - val_RMSE: 0.8183 - val_MAE: 0.3707\n",
      "Epoch 32/200\n",
      "138/138 - 1s - loss: 0.4699 - MSE: 0.4699 - RMSE: 0.6855 - MAE: 0.3682 - val_loss: 0.6532 - val_MSE: 0.6532 - val_RMSE: 0.8082 - val_MAE: 0.3662\n",
      "Epoch 33/200\n",
      "138/138 - 1s - loss: 0.4574 - MSE: 0.4574 - RMSE: 0.6763 - MAE: 0.3628 - val_loss: 0.6316 - val_MSE: 0.6316 - val_RMSE: 0.7947 - val_MAE: 0.3619\n",
      "Epoch 34/200\n",
      "138/138 - 1s - loss: 0.4434 - MSE: 0.4434 - RMSE: 0.6659 - MAE: 0.3588 - val_loss: 0.6123 - val_MSE: 0.6123 - val_RMSE: 0.7825 - val_MAE: 0.3575\n",
      "Epoch 35/200\n",
      "138/138 - 1s - loss: 0.4321 - MSE: 0.4321 - RMSE: 0.6573 - MAE: 0.3547 - val_loss: 0.5962 - val_MSE: 0.5962 - val_RMSE: 0.7721 - val_MAE: 0.3530\n",
      "Epoch 36/200\n",
      "138/138 - 1s - loss: 0.4209 - MSE: 0.4209 - RMSE: 0.6488 - MAE: 0.3511 - val_loss: 0.5819 - val_MSE: 0.5819 - val_RMSE: 0.7628 - val_MAE: 0.3494\n",
      "Epoch 37/200\n",
      "138/138 - 1s - loss: 0.4103 - MSE: 0.4103 - RMSE: 0.6405 - MAE: 0.3467 - val_loss: 0.5649 - val_MSE: 0.5649 - val_RMSE: 0.7516 - val_MAE: 0.3452\n",
      "Epoch 38/200\n",
      "138/138 - 1s - loss: 0.3993 - MSE: 0.3993 - RMSE: 0.6319 - MAE: 0.3426 - val_loss: 0.5508 - val_MSE: 0.5508 - val_RMSE: 0.7421 - val_MAE: 0.3408\n",
      "Epoch 39/200\n",
      "138/138 - 1s - loss: 0.3896 - MSE: 0.3896 - RMSE: 0.6242 - MAE: 0.3383 - val_loss: 0.5387 - val_MSE: 0.5387 - val_RMSE: 0.7339 - val_MAE: 0.3368\n",
      "Epoch 40/200\n",
      "138/138 - 1s - loss: 0.3803 - MSE: 0.3803 - RMSE: 0.6167 - MAE: 0.3344 - val_loss: 0.5249 - val_MSE: 0.5249 - val_RMSE: 0.7245 - val_MAE: 0.3333\n",
      "Epoch 41/200\n",
      "138/138 - 1s - loss: 0.3716 - MSE: 0.3716 - RMSE: 0.6096 - MAE: 0.3308 - val_loss: 0.5134 - val_MSE: 0.5134 - val_RMSE: 0.7165 - val_MAE: 0.3292\n",
      "Epoch 42/200\n",
      "138/138 - 1s - loss: 0.3625 - MSE: 0.3625 - RMSE: 0.6021 - MAE: 0.3266 - val_loss: 0.5053 - val_MSE: 0.5053 - val_RMSE: 0.7108 - val_MAE: 0.3245\n",
      "Epoch 43/200\n",
      "138/138 - 2s - loss: 0.3537 - MSE: 0.3537 - RMSE: 0.5947 - MAE: 0.3226 - val_loss: 0.4886 - val_MSE: 0.4886 - val_RMSE: 0.6990 - val_MAE: 0.3211\n",
      "Epoch 44/200\n",
      "138/138 - 1s - loss: 0.3453 - MSE: 0.3453 - RMSE: 0.5877 - MAE: 0.3191 - val_loss: 0.4776 - val_MSE: 0.4776 - val_RMSE: 0.6911 - val_MAE: 0.3170\n",
      "Epoch 45/200\n",
      "138/138 - 2s - loss: 0.3377 - MSE: 0.3377 - RMSE: 0.5811 - MAE: 0.3149 - val_loss: 0.4686 - val_MSE: 0.4686 - val_RMSE: 0.6846 - val_MAE: 0.3138\n",
      "Epoch 46/200\n",
      "138/138 - 1s - loss: 0.3306 - MSE: 0.3306 - RMSE: 0.5750 - MAE: 0.3118 - val_loss: 0.4564 - val_MSE: 0.4564 - val_RMSE: 0.6756 - val_MAE: 0.3098\n",
      "Epoch 47/200\n",
      "138/138 - 1s - loss: 0.3227 - MSE: 0.3227 - RMSE: 0.5681 - MAE: 0.3079 - val_loss: 0.4474 - val_MSE: 0.4474 - val_RMSE: 0.6689 - val_MAE: 0.3066\n",
      "Epoch 48/200\n",
      "138/138 - 1s - loss: 0.3156 - MSE: 0.3156 - RMSE: 0.5618 - MAE: 0.3044 - val_loss: 0.4360 - val_MSE: 0.4360 - val_RMSE: 0.6603 - val_MAE: 0.3031\n",
      "Epoch 49/200\n",
      "138/138 - 1s - loss: 0.3084 - MSE: 0.3084 - RMSE: 0.5553 - MAE: 0.3017 - val_loss: 0.4280 - val_MSE: 0.4280 - val_RMSE: 0.6542 - val_MAE: 0.3005\n",
      "Epoch 50/200\n",
      "138/138 - 1s - loss: 0.3020 - MSE: 0.3020 - RMSE: 0.5495 - MAE: 0.2986 - val_loss: 0.4188 - val_MSE: 0.4188 - val_RMSE: 0.6472 - val_MAE: 0.2971\n",
      "Epoch 51/200\n",
      "138/138 - 1s - loss: 0.2954 - MSE: 0.2954 - RMSE: 0.5435 - MAE: 0.2951 - val_loss: 0.4091 - val_MSE: 0.4091 - val_RMSE: 0.6396 - val_MAE: 0.2940\n",
      "Epoch 52/200\n",
      "138/138 - 1s - loss: 0.2893 - MSE: 0.2893 - RMSE: 0.5378 - MAE: 0.2926 - val_loss: 0.4012 - val_MSE: 0.4012 - val_RMSE: 0.6334 - val_MAE: 0.2910\n",
      "Epoch 53/200\n",
      "138/138 - 1s - loss: 0.2827 - MSE: 0.2827 - RMSE: 0.5317 - MAE: 0.2888 - val_loss: 0.3913 - val_MSE: 0.3913 - val_RMSE: 0.6256 - val_MAE: 0.2878\n",
      "Epoch 54/200\n",
      "138/138 - 1s - loss: 0.2759 - MSE: 0.2759 - RMSE: 0.5252 - MAE: 0.2860 - val_loss: 0.3821 - val_MSE: 0.3821 - val_RMSE: 0.6182 - val_MAE: 0.2847\n",
      "Epoch 55/200\n",
      "138/138 - 1s - loss: 0.2698 - MSE: 0.2698 - RMSE: 0.5195 - MAE: 0.2830 - val_loss: 0.3732 - val_MSE: 0.3732 - val_RMSE: 0.6109 - val_MAE: 0.2817\n",
      "Epoch 56/200\n",
      "138/138 - 1s - loss: 0.2635 - MSE: 0.2635 - RMSE: 0.5133 - MAE: 0.2799 - val_loss: 0.3671 - val_MSE: 0.3671 - val_RMSE: 0.6059 - val_MAE: 0.2785\n",
      "Epoch 57/200\n",
      "138/138 - 1s - loss: 0.2591 - MSE: 0.2591 - RMSE: 0.5090 - MAE: 0.2774 - val_loss: 0.3593 - val_MSE: 0.3593 - val_RMSE: 0.5994 - val_MAE: 0.2758\n",
      "Epoch 58/200\n",
      "138/138 - 1s - loss: 0.2523 - MSE: 0.2523 - RMSE: 0.5023 - MAE: 0.2746 - val_loss: 0.3483 - val_MSE: 0.3483 - val_RMSE: 0.5902 - val_MAE: 0.2742\n",
      "Epoch 59/200\n",
      "138/138 - 1s - loss: 0.2469 - MSE: 0.2469 - RMSE: 0.4969 - MAE: 0.2721 - val_loss: 0.3394 - val_MSE: 0.3394 - val_RMSE: 0.5825 - val_MAE: 0.2710\n",
      "Epoch 60/200\n",
      "138/138 - 1s - loss: 0.2407 - MSE: 0.2407 - RMSE: 0.4906 - MAE: 0.2692 - val_loss: 0.3340 - val_MSE: 0.3340 - val_RMSE: 0.5780 - val_MAE: 0.2678\n",
      "Epoch 61/200\n",
      "138/138 - 1s - loss: 0.2351 - MSE: 0.2351 - RMSE: 0.4849 - MAE: 0.2667 - val_loss: 0.3231 - val_MSE: 0.3231 - val_RMSE: 0.5684 - val_MAE: 0.2654\n",
      "Epoch 62/200\n",
      "138/138 - 1s - loss: 0.2296 - MSE: 0.2296 - RMSE: 0.4791 - MAE: 0.2639 - val_loss: 0.3158 - val_MSE: 0.3158 - val_RMSE: 0.5620 - val_MAE: 0.2625\n",
      "Epoch 63/200\n",
      "138/138 - 1s - loss: 0.2246 - MSE: 0.2246 - RMSE: 0.4739 - MAE: 0.2613 - val_loss: 0.3108 - val_MSE: 0.3108 - val_RMSE: 0.5575 - val_MAE: 0.2606\n",
      "Epoch 64/200\n",
      "138/138 - 1s - loss: 0.2206 - MSE: 0.2206 - RMSE: 0.4696 - MAE: 0.2590 - val_loss: 0.3003 - val_MSE: 0.3003 - val_RMSE: 0.5480 - val_MAE: 0.2577\n",
      "Epoch 65/200\n",
      "138/138 - 1s - loss: 0.2142 - MSE: 0.2142 - RMSE: 0.4629 - MAE: 0.2563 - val_loss: 0.2934 - val_MSE: 0.2934 - val_RMSE: 0.5417 - val_MAE: 0.2551\n",
      "Epoch 66/200\n",
      "138/138 - 1s - loss: 0.2097 - MSE: 0.2097 - RMSE: 0.4580 - MAE: 0.2538 - val_loss: 0.2870 - val_MSE: 0.2870 - val_RMSE: 0.5357 - val_MAE: 0.2526\n",
      "Epoch 67/200\n",
      "138/138 - 1s - loss: 0.2048 - MSE: 0.2048 - RMSE: 0.4526 - MAE: 0.2511 - val_loss: 0.2789 - val_MSE: 0.2789 - val_RMSE: 0.5281 - val_MAE: 0.2502\n",
      "Epoch 68/200\n",
      "138/138 - 1s - loss: 0.1998 - MSE: 0.1998 - RMSE: 0.4470 - MAE: 0.2490 - val_loss: 0.2720 - val_MSE: 0.2720 - val_RMSE: 0.5216 - val_MAE: 0.2478\n",
      "Epoch 69/200\n",
      "138/138 - 1s - loss: 0.1953 - MSE: 0.1953 - RMSE: 0.4420 - MAE: 0.2466 - val_loss: 0.2655 - val_MSE: 0.2655 - val_RMSE: 0.5153 - val_MAE: 0.2451\n",
      "Epoch 70/200\n",
      "138/138 - 1s - loss: 0.1910 - MSE: 0.1910 - RMSE: 0.4370 - MAE: 0.2439 - val_loss: 0.2592 - val_MSE: 0.2592 - val_RMSE: 0.5091 - val_MAE: 0.2431\n",
      "Epoch 71/200\n",
      "138/138 - 1s - loss: 0.1865 - MSE: 0.1865 - RMSE: 0.4319 - MAE: 0.2416 - val_loss: 0.2556 - val_MSE: 0.2556 - val_RMSE: 0.5056 - val_MAE: 0.2406\n",
      "Epoch 72/200\n",
      "138/138 - 1s - loss: 0.1844 - MSE: 0.1844 - RMSE: 0.4294 - MAE: 0.2396 - val_loss: 0.2468 - val_MSE: 0.2468 - val_RMSE: 0.4968 - val_MAE: 0.2390\n",
      "Epoch 73/200\n",
      "138/138 - 1s - loss: 0.1787 - MSE: 0.1787 - RMSE: 0.4228 - MAE: 0.2374 - val_loss: 0.2398 - val_MSE: 0.2398 - val_RMSE: 0.4897 - val_MAE: 0.2364\n",
      "Epoch 74/200\n",
      "138/138 - 1s - loss: 0.1745 - MSE: 0.1745 - RMSE: 0.4178 - MAE: 0.2351 - val_loss: 0.2334 - val_MSE: 0.2334 - val_RMSE: 0.4831 - val_MAE: 0.2342\n",
      "Epoch 75/200\n",
      "138/138 - 1s - loss: 0.1706 - MSE: 0.1706 - RMSE: 0.4131 - MAE: 0.2331 - val_loss: 0.2275 - val_MSE: 0.2275 - val_RMSE: 0.4770 - val_MAE: 0.2318\n",
      "Epoch 76/200\n",
      "138/138 - 1s - loss: 0.1668 - MSE: 0.1668 - RMSE: 0.4085 - MAE: 0.2310 - val_loss: 0.2214 - val_MSE: 0.2214 - val_RMSE: 0.4706 - val_MAE: 0.2299\n",
      "Epoch 77/200\n",
      "138/138 - 1s - loss: 0.1633 - MSE: 0.1633 - RMSE: 0.4041 - MAE: 0.2289 - val_loss: 0.2164 - val_MSE: 0.2164 - val_RMSE: 0.4652 - val_MAE: 0.2278\n",
      "Epoch 78/200\n",
      "138/138 - 1s - loss: 0.1596 - MSE: 0.1596 - RMSE: 0.3995 - MAE: 0.2270 - val_loss: 0.2112 - val_MSE: 0.2112 - val_RMSE: 0.4595 - val_MAE: 0.2264\n",
      "Epoch 79/200\n",
      "138/138 - 1s - loss: 0.1564 - MSE: 0.1564 - RMSE: 0.3955 - MAE: 0.2250 - val_loss: 0.2043 - val_MSE: 0.2043 - val_RMSE: 0.4520 - val_MAE: 0.2237\n",
      "Epoch 80/200\n",
      "138/138 - 1s - loss: 0.1525 - MSE: 0.1525 - RMSE: 0.3905 - MAE: 0.2227 - val_loss: 0.1996 - val_MSE: 0.1996 - val_RMSE: 0.4468 - val_MAE: 0.2214\n",
      "Epoch 81/200\n",
      "138/138 - 1s - loss: 0.1493 - MSE: 0.1493 - RMSE: 0.3863 - MAE: 0.2204 - val_loss: 0.1954 - val_MSE: 0.1954 - val_RMSE: 0.4420 - val_MAE: 0.2193\n",
      "Epoch 82/200\n",
      "138/138 - 1s - loss: 0.1465 - MSE: 0.1465 - RMSE: 0.3827 - MAE: 0.2185 - val_loss: 0.1893 - val_MSE: 0.1893 - val_RMSE: 0.4351 - val_MAE: 0.2171\n",
      "Epoch 83/200\n",
      "138/138 - 1s - loss: 0.1430 - MSE: 0.1430 - RMSE: 0.3781 - MAE: 0.2162 - val_loss: 0.1844 - val_MSE: 0.1844 - val_RMSE: 0.4294 - val_MAE: 0.2153\n",
      "Epoch 84/200\n",
      "138/138 - 1s - loss: 0.1399 - MSE: 0.1399 - RMSE: 0.3740 - MAE: 0.2144 - val_loss: 0.1793 - val_MSE: 0.1793 - val_RMSE: 0.4235 - val_MAE: 0.2132\n",
      "Epoch 85/200\n",
      "138/138 - 1s - loss: 0.1365 - MSE: 0.1365 - RMSE: 0.3694 - MAE: 0.2120 - val_loss: 0.1746 - val_MSE: 0.1746 - val_RMSE: 0.4179 - val_MAE: 0.2110\n",
      "Epoch 86/200\n",
      "138/138 - 1s - loss: 0.1336 - MSE: 0.1336 - RMSE: 0.3655 - MAE: 0.2100 - val_loss: 0.1696 - val_MSE: 0.1696 - val_RMSE: 0.4118 - val_MAE: 0.2090\n",
      "Epoch 87/200\n",
      "138/138 - 1s - loss: 0.1308 - MSE: 0.1308 - RMSE: 0.3617 - MAE: 0.2083 - val_loss: 0.1670 - val_MSE: 0.1670 - val_RMSE: 0.4087 - val_MAE: 0.2069\n",
      "Epoch 88/200\n",
      "138/138 - 1s - loss: 0.1284 - MSE: 0.1284 - RMSE: 0.3583 - MAE: 0.2063 - val_loss: 0.1624 - val_MSE: 0.1624 - val_RMSE: 0.4030 - val_MAE: 0.2062\n",
      "Epoch 89/200\n",
      "138/138 - 1s - loss: 0.1255 - MSE: 0.1255 - RMSE: 0.3543 - MAE: 0.2044 - val_loss: 0.1587 - val_MSE: 0.1587 - val_RMSE: 0.3984 - val_MAE: 0.2039\n",
      "Epoch 90/200\n",
      "138/138 - 1s - loss: 0.1227 - MSE: 0.1227 - RMSE: 0.3503 - MAE: 0.2028 - val_loss: 0.1536 - val_MSE: 0.1536 - val_RMSE: 0.3919 - val_MAE: 0.2022\n",
      "Epoch 91/200\n",
      "138/138 - 1s - loss: 0.1205 - MSE: 0.1205 - RMSE: 0.3471 - MAE: 0.2010 - val_loss: 0.1498 - val_MSE: 0.1498 - val_RMSE: 0.3870 - val_MAE: 0.2008\n",
      "Epoch 92/200\n",
      "138/138 - 1s - loss: 0.1181 - MSE: 0.1181 - RMSE: 0.3437 - MAE: 0.1996 - val_loss: 0.1455 - val_MSE: 0.1455 - val_RMSE: 0.3814 - val_MAE: 0.1983\n",
      "Epoch 93/200\n",
      "138/138 - 1s - loss: 0.1157 - MSE: 0.1157 - RMSE: 0.3401 - MAE: 0.1977 - val_loss: 0.1426 - val_MSE: 0.1426 - val_RMSE: 0.3776 - val_MAE: 0.1973\n",
      "Epoch 94/200\n",
      "138/138 - 1s - loss: 0.1131 - MSE: 0.1131 - RMSE: 0.3363 - MAE: 0.1962 - val_loss: 0.1401 - val_MSE: 0.1401 - val_RMSE: 0.3742 - val_MAE: 0.1952\n",
      "Epoch 95/200\n",
      "138/138 - 1s - loss: 0.1114 - MSE: 0.1114 - RMSE: 0.3338 - MAE: 0.1950 - val_loss: 0.1352 - val_MSE: 0.1352 - val_RMSE: 0.3676 - val_MAE: 0.1947\n",
      "Epoch 96/200\n",
      "138/138 - 1s - loss: 0.1088 - MSE: 0.1088 - RMSE: 0.3299 - MAE: 0.1933 - val_loss: 0.1305 - val_MSE: 0.1305 - val_RMSE: 0.3612 - val_MAE: 0.1924\n",
      "Epoch 97/200\n",
      "138/138 - 1s - loss: 0.1067 - MSE: 0.1067 - RMSE: 0.3266 - MAE: 0.1919 - val_loss: 0.1265 - val_MSE: 0.1265 - val_RMSE: 0.3557 - val_MAE: 0.1911\n",
      "Epoch 98/200\n",
      "138/138 - 1s - loss: 0.1042 - MSE: 0.1042 - RMSE: 0.3228 - MAE: 0.1908 - val_loss: 0.1242 - val_MSE: 0.1242 - val_RMSE: 0.3525 - val_MAE: 0.1896\n",
      "Epoch 99/200\n",
      "138/138 - 1s - loss: 0.1025 - MSE: 0.1025 - RMSE: 0.3201 - MAE: 0.1894 - val_loss: 0.1206 - val_MSE: 0.1206 - val_RMSE: 0.3472 - val_MAE: 0.1882\n",
      "Epoch 100/200\n",
      "138/138 - 1s - loss: 0.1000 - MSE: 0.1000 - RMSE: 0.3162 - MAE: 0.1878 - val_loss: 0.1173 - val_MSE: 0.1173 - val_RMSE: 0.3426 - val_MAE: 0.1869\n",
      "Epoch 101/200\n",
      "138/138 - 1s - loss: 0.0982 - MSE: 0.0982 - RMSE: 0.3133 - MAE: 0.1862 - val_loss: 0.1150 - val_MSE: 0.1150 - val_RMSE: 0.3391 - val_MAE: 0.1860\n",
      "Epoch 102/200\n",
      "138/138 - 1s - loss: 0.0961 - MSE: 0.0961 - RMSE: 0.3100 - MAE: 0.1855 - val_loss: 0.1110 - val_MSE: 0.1110 - val_RMSE: 0.3331 - val_MAE: 0.1847\n",
      "Epoch 103/200\n",
      "138/138 - 1s - loss: 0.0944 - MSE: 0.0944 - RMSE: 0.3073 - MAE: 0.1842 - val_loss: 0.1081 - val_MSE: 0.1081 - val_RMSE: 0.3288 - val_MAE: 0.1834\n",
      "Epoch 104/200\n",
      "138/138 - 1s - loss: 0.0925 - MSE: 0.0925 - RMSE: 0.3041 - MAE: 0.1827 - val_loss: 0.1053 - val_MSE: 0.1053 - val_RMSE: 0.3245 - val_MAE: 0.1831\n",
      "Epoch 105/200\n",
      "138/138 - 1s - loss: 0.0908 - MSE: 0.0908 - RMSE: 0.3013 - MAE: 0.1820 - val_loss: 0.1020 - val_MSE: 0.1020 - val_RMSE: 0.3193 - val_MAE: 0.1810\n",
      "Epoch 106/200\n",
      "138/138 - 1s - loss: 0.0889 - MSE: 0.0889 - RMSE: 0.2982 - MAE: 0.1808 - val_loss: 0.0997 - val_MSE: 0.0997 - val_RMSE: 0.3158 - val_MAE: 0.1800\n",
      "Epoch 107/200\n",
      "138/138 - 1s - loss: 0.0871 - MSE: 0.0871 - RMSE: 0.2952 - MAE: 0.1797 - val_loss: 0.0967 - val_MSE: 0.0967 - val_RMSE: 0.3109 - val_MAE: 0.1788\n",
      "Epoch 108/200\n",
      "138/138 - 1s - loss: 0.0854 - MSE: 0.0854 - RMSE: 0.2923 - MAE: 0.1785 - val_loss: 0.0942 - val_MSE: 0.0942 - val_RMSE: 0.3070 - val_MAE: 0.1776\n",
      "Epoch 109/200\n",
      "138/138 - 1s - loss: 0.0839 - MSE: 0.0839 - RMSE: 0.2897 - MAE: 0.1775 - val_loss: 0.0923 - val_MSE: 0.0923 - val_RMSE: 0.3038 - val_MAE: 0.1766\n",
      "Epoch 110/200\n",
      "138/138 - 1s - loss: 0.0825 - MSE: 0.0825 - RMSE: 0.2873 - MAE: 0.1765 - val_loss: 0.0900 - val_MSE: 0.0900 - val_RMSE: 0.3000 - val_MAE: 0.1757\n",
      "Epoch 111/200\n",
      "138/138 - 1s - loss: 0.0810 - MSE: 0.0810 - RMSE: 0.2846 - MAE: 0.1754 - val_loss: 0.0873 - val_MSE: 0.0873 - val_RMSE: 0.2955 - val_MAE: 0.1746\n",
      "Epoch 112/200\n",
      "138/138 - 1s - loss: 0.0793 - MSE: 0.0793 - RMSE: 0.2817 - MAE: 0.1743 - val_loss: 0.0855 - val_MSE: 0.0855 - val_RMSE: 0.2924 - val_MAE: 0.1738\n",
      "Epoch 113/200\n",
      "138/138 - 1s - loss: 0.0780 - MSE: 0.0780 - RMSE: 0.2793 - MAE: 0.1734 - val_loss: 0.0830 - val_MSE: 0.0830 - val_RMSE: 0.2882 - val_MAE: 0.1725\n",
      "Epoch 114/200\n",
      "138/138 - 1s - loss: 0.0767 - MSE: 0.0767 - RMSE: 0.2770 - MAE: 0.1724 - val_loss: 0.0814 - val_MSE: 0.0814 - val_RMSE: 0.2854 - val_MAE: 0.1719\n",
      "Epoch 115/200\n",
      "138/138 - 1s - loss: 0.0755 - MSE: 0.0755 - RMSE: 0.2747 - MAE: 0.1714 - val_loss: 0.0795 - val_MSE: 0.0795 - val_RMSE: 0.2820 - val_MAE: 0.1707\n",
      "Epoch 116/200\n",
      "138/138 - 1s - loss: 0.0742 - MSE: 0.0742 - RMSE: 0.2724 - MAE: 0.1702 - val_loss: 0.0795 - val_MSE: 0.0795 - val_RMSE: 0.2819 - val_MAE: 0.1725\n",
      "Epoch 117/200\n",
      "138/138 - 1s - loss: 0.0732 - MSE: 0.0732 - RMSE: 0.2705 - MAE: 0.1696 - val_loss: 0.0765 - val_MSE: 0.0765 - val_RMSE: 0.2765 - val_MAE: 0.1686\n",
      "Epoch 118/200\n",
      "138/138 - 1s - loss: 0.0718 - MSE: 0.0718 - RMSE: 0.2679 - MAE: 0.1683 - val_loss: 0.0746 - val_MSE: 0.0746 - val_RMSE: 0.2731 - val_MAE: 0.1674\n",
      "Epoch 119/200\n",
      "138/138 - 1s - loss: 0.0707 - MSE: 0.0707 - RMSE: 0.2659 - MAE: 0.1674 - val_loss: 0.0735 - val_MSE: 0.0735 - val_RMSE: 0.2711 - val_MAE: 0.1669\n",
      "Epoch 120/200\n",
      "138/138 - 1s - loss: 0.0698 - MSE: 0.0698 - RMSE: 0.2643 - MAE: 0.1666 - val_loss: 0.0722 - val_MSE: 0.0722 - val_RMSE: 0.2687 - val_MAE: 0.1657\n",
      "Epoch 121/200\n",
      "138/138 - 1s - loss: 0.0687 - MSE: 0.0687 - RMSE: 0.2620 - MAE: 0.1654 - val_loss: 0.0716 - val_MSE: 0.0716 - val_RMSE: 0.2676 - val_MAE: 0.1675\n",
      "Epoch 122/200\n",
      "138/138 - 1s - loss: 0.0682 - MSE: 0.0682 - RMSE: 0.2611 - MAE: 0.1650 - val_loss: 0.0698 - val_MSE: 0.0698 - val_RMSE: 0.2642 - val_MAE: 0.1642\n",
      "Epoch 123/200\n",
      "138/138 - 1s - loss: 0.0671 - MSE: 0.0671 - RMSE: 0.2590 - MAE: 0.1639 - val_loss: 0.0683 - val_MSE: 0.0683 - val_RMSE: 0.2614 - val_MAE: 0.1628\n",
      "Epoch 124/200\n",
      "138/138 - 1s - loss: 0.0659 - MSE: 0.0659 - RMSE: 0.2567 - MAE: 0.1626 - val_loss: 0.0685 - val_MSE: 0.0685 - val_RMSE: 0.2617 - val_MAE: 0.1648\n",
      "Epoch 125/200\n",
      "138/138 - 1s - loss: 0.0651 - MSE: 0.0651 - RMSE: 0.2552 - MAE: 0.1622 - val_loss: 0.0657 - val_MSE: 0.0657 - val_RMSE: 0.2563 - val_MAE: 0.1611\n",
      "Epoch 126/200\n",
      "138/138 - 1s - loss: 0.0641 - MSE: 0.0641 - RMSE: 0.2532 - MAE: 0.1608 - val_loss: 0.0651 - val_MSE: 0.0651 - val_RMSE: 0.2551 - val_MAE: 0.1607\n",
      "Epoch 127/200\n",
      "138/138 - 1s - loss: 0.0635 - MSE: 0.0635 - RMSE: 0.2520 - MAE: 0.1603 - val_loss: 0.0643 - val_MSE: 0.0643 - val_RMSE: 0.2535 - val_MAE: 0.1597\n",
      "Epoch 128/200\n",
      "138/138 - 1s - loss: 0.0626 - MSE: 0.0626 - RMSE: 0.2502 - MAE: 0.1594 - val_loss: 0.0633 - val_MSE: 0.0633 - val_RMSE: 0.2517 - val_MAE: 0.1586\n",
      "Epoch 129/200\n",
      "138/138 - 1s - loss: 0.0618 - MSE: 0.0618 - RMSE: 0.2486 - MAE: 0.1585 - val_loss: 0.0623 - val_MSE: 0.0623 - val_RMSE: 0.2497 - val_MAE: 0.1579\n",
      "Epoch 130/200\n",
      "138/138 - 1s - loss: 0.0611 - MSE: 0.0611 - RMSE: 0.2472 - MAE: 0.1577 - val_loss: 0.0621 - val_MSE: 0.0621 - val_RMSE: 0.2491 - val_MAE: 0.1570\n",
      "Epoch 131/200\n",
      "138/138 - 1s - loss: 0.0603 - MSE: 0.0603 - RMSE: 0.2455 - MAE: 0.1566 - val_loss: 0.0613 - val_MSE: 0.0613 - val_RMSE: 0.2476 - val_MAE: 0.1570\n",
      "Epoch 132/200\n",
      "138/138 - 1s - loss: 0.0598 - MSE: 0.0598 - RMSE: 0.2446 - MAE: 0.1561 - val_loss: 0.0604 - val_MSE: 0.0604 - val_RMSE: 0.2457 - val_MAE: 0.1551\n",
      "Epoch 133/200\n",
      "138/138 - 1s - loss: 0.0591 - MSE: 0.0591 - RMSE: 0.2432 - MAE: 0.1553 - val_loss: 0.0590 - val_MSE: 0.0590 - val_RMSE: 0.2429 - val_MAE: 0.1542\n",
      "Epoch 134/200\n",
      "138/138 - 1s - loss: 0.0583 - MSE: 0.0583 - RMSE: 0.2415 - MAE: 0.1541 - val_loss: 0.0584 - val_MSE: 0.0584 - val_RMSE: 0.2417 - val_MAE: 0.1535\n",
      "Epoch 135/200\n",
      "138/138 - 1s - loss: 0.0575 - MSE: 0.0575 - RMSE: 0.2399 - MAE: 0.1533 - val_loss: 0.0581 - val_MSE: 0.0581 - val_RMSE: 0.2410 - val_MAE: 0.1530\n",
      "Epoch 136/200\n",
      "138/138 - 1s - loss: 0.0571 - MSE: 0.0571 - RMSE: 0.2389 - MAE: 0.1526 - val_loss: 0.0575 - val_MSE: 0.0575 - val_RMSE: 0.2398 - val_MAE: 0.1519\n",
      "Epoch 137/200\n",
      "138/138 - 1s - loss: 0.0565 - MSE: 0.0565 - RMSE: 0.2377 - MAE: 0.1519 - val_loss: 0.0566 - val_MSE: 0.0566 - val_RMSE: 0.2378 - val_MAE: 0.1509\n",
      "Epoch 138/200\n",
      "138/138 - 1s - loss: 0.0558 - MSE: 0.0558 - RMSE: 0.2363 - MAE: 0.1509 - val_loss: 0.0562 - val_MSE: 0.0562 - val_RMSE: 0.2371 - val_MAE: 0.1500\n",
      "Epoch 139/200\n",
      "138/138 - 1s - loss: 0.0554 - MSE: 0.0554 - RMSE: 0.2354 - MAE: 0.1504 - val_loss: 0.0557 - val_MSE: 0.0557 - val_RMSE: 0.2360 - val_MAE: 0.1493\n",
      "Epoch 140/200\n",
      "138/138 - 1s - loss: 0.0548 - MSE: 0.0548 - RMSE: 0.2340 - MAE: 0.1492 - val_loss: 0.0573 - val_MSE: 0.0573 - val_RMSE: 0.2395 - val_MAE: 0.1527\n",
      "Epoch 141/200\n",
      "138/138 - 1s - loss: 0.0550 - MSE: 0.0550 - RMSE: 0.2345 - MAE: 0.1494 - val_loss: 0.0551 - val_MSE: 0.0551 - val_RMSE: 0.2346 - val_MAE: 0.1477\n",
      "Epoch 142/200\n",
      "138/138 - 1s - loss: 0.0538 - MSE: 0.0538 - RMSE: 0.2319 - MAE: 0.1476 - val_loss: 0.0566 - val_MSE: 0.0566 - val_RMSE: 0.2378 - val_MAE: 0.1531\n",
      "Epoch 143/200\n",
      "138/138 - 1s - loss: 0.0537 - MSE: 0.0537 - RMSE: 0.2316 - MAE: 0.1476 - val_loss: 0.0535 - val_MSE: 0.0535 - val_RMSE: 0.2313 - val_MAE: 0.1460\n",
      "Epoch 144/200\n",
      "138/138 - 1s - loss: 0.0529 - MSE: 0.0529 - RMSE: 0.2299 - MAE: 0.1462 - val_loss: 0.0532 - val_MSE: 0.0532 - val_RMSE: 0.2307 - val_MAE: 0.1459\n",
      "Epoch 145/200\n",
      "138/138 - 1s - loss: 0.0523 - MSE: 0.0523 - RMSE: 0.2286 - MAE: 0.1456 - val_loss: 0.0520 - val_MSE: 0.0520 - val_RMSE: 0.2281 - val_MAE: 0.1444\n",
      "Epoch 146/200\n",
      "138/138 - 1s - loss: 0.0518 - MSE: 0.0518 - RMSE: 0.2276 - MAE: 0.1448 - val_loss: 0.0513 - val_MSE: 0.0513 - val_RMSE: 0.2266 - val_MAE: 0.1441\n",
      "Epoch 147/200\n",
      "138/138 - 1s - loss: 0.0510 - MSE: 0.0510 - RMSE: 0.2258 - MAE: 0.1435 - val_loss: 0.0517 - val_MSE: 0.0517 - val_RMSE: 0.2273 - val_MAE: 0.1456\n",
      "Epoch 148/200\n",
      "138/138 - 1s - loss: 0.0506 - MSE: 0.0506 - RMSE: 0.2249 - MAE: 0.1432 - val_loss: 0.0502 - val_MSE: 0.0502 - val_RMSE: 0.2241 - val_MAE: 0.1421\n",
      "Epoch 149/200\n",
      "138/138 - 1s - loss: 0.0502 - MSE: 0.0502 - RMSE: 0.2240 - MAE: 0.1422 - val_loss: 0.0509 - val_MSE: 0.0509 - val_RMSE: 0.2256 - val_MAE: 0.1417\n",
      "Epoch 150/200\n",
      "138/138 - 1s - loss: 0.0499 - MSE: 0.0499 - RMSE: 0.2235 - MAE: 0.1417 - val_loss: 0.0498 - val_MSE: 0.0498 - val_RMSE: 0.2231 - val_MAE: 0.1407\n",
      "Epoch 151/200\n",
      "138/138 - 1s - loss: 0.0494 - MSE: 0.0494 - RMSE: 0.2223 - MAE: 0.1410 - val_loss: 0.0491 - val_MSE: 0.0491 - val_RMSE: 0.2216 - val_MAE: 0.1403\n",
      "Epoch 152/200\n",
      "138/138 - 1s - loss: 0.0489 - MSE: 0.0489 - RMSE: 0.2212 - MAE: 0.1402 - val_loss: 0.0486 - val_MSE: 0.0486 - val_RMSE: 0.2204 - val_MAE: 0.1397\n",
      "Epoch 153/200\n",
      "138/138 - 1s - loss: 0.0485 - MSE: 0.0485 - RMSE: 0.2203 - MAE: 0.1395 - val_loss: 0.0488 - val_MSE: 0.0488 - val_RMSE: 0.2210 - val_MAE: 0.1395\n",
      "Epoch 154/200\n",
      "138/138 - 1s - loss: 0.0482 - MSE: 0.0482 - RMSE: 0.2196 - MAE: 0.1389 - val_loss: 0.0480 - val_MSE: 0.0480 - val_RMSE: 0.2192 - val_MAE: 0.1380\n",
      "Epoch 155/200\n",
      "138/138 - 1s - loss: 0.0477 - MSE: 0.0477 - RMSE: 0.2184 - MAE: 0.1380 - val_loss: 0.0482 - val_MSE: 0.0482 - val_RMSE: 0.2196 - val_MAE: 0.1386\n",
      "Epoch 156/200\n",
      "138/138 - 1s - loss: 0.0475 - MSE: 0.0475 - RMSE: 0.2180 - MAE: 0.1378 - val_loss: 0.0473 - val_MSE: 0.0473 - val_RMSE: 0.2175 - val_MAE: 0.1368\n",
      "Epoch 157/200\n",
      "138/138 - 1s - loss: 0.0470 - MSE: 0.0470 - RMSE: 0.2168 - MAE: 0.1370 - val_loss: 0.0466 - val_MSE: 0.0466 - val_RMSE: 0.2158 - val_MAE: 0.1363\n",
      "Epoch 158/200\n",
      "138/138 - 1s - loss: 0.0466 - MSE: 0.0466 - RMSE: 0.2159 - MAE: 0.1364 - val_loss: 0.0464 - val_MSE: 0.0464 - val_RMSE: 0.2155 - val_MAE: 0.1355\n",
      "Epoch 159/200\n",
      "138/138 - 1s - loss: 0.0461 - MSE: 0.0461 - RMSE: 0.2147 - MAE: 0.1356 - val_loss: 0.0459 - val_MSE: 0.0459 - val_RMSE: 0.2142 - val_MAE: 0.1350\n",
      "Epoch 160/200\n",
      "138/138 - 1s - loss: 0.0458 - MSE: 0.0458 - RMSE: 0.2139 - MAE: 0.1352 - val_loss: 0.0461 - val_MSE: 0.0461 - val_RMSE: 0.2148 - val_MAE: 0.1352\n",
      "Epoch 161/200\n",
      "138/138 - 1s - loss: 0.0453 - MSE: 0.0453 - RMSE: 0.2130 - MAE: 0.1343 - val_loss: 0.0458 - val_MSE: 0.0458 - val_RMSE: 0.2140 - val_MAE: 0.1358\n",
      "Epoch 162/200\n",
      "138/138 - 1s - loss: 0.0455 - MSE: 0.0455 - RMSE: 0.2132 - MAE: 0.1345 - val_loss: 0.0457 - val_MSE: 0.0457 - val_RMSE: 0.2139 - val_MAE: 0.1331\n",
      "Epoch 163/200\n",
      "138/138 - 1s - loss: 0.0451 - MSE: 0.0451 - RMSE: 0.2123 - MAE: 0.1335 - val_loss: 0.0464 - val_MSE: 0.0464 - val_RMSE: 0.2155 - val_MAE: 0.1334\n",
      "Epoch 164/200\n",
      "138/138 - 1s - loss: 0.0452 - MSE: 0.0452 - RMSE: 0.2126 - MAE: 0.1335 - val_loss: 0.0457 - val_MSE: 0.0457 - val_RMSE: 0.2138 - val_MAE: 0.1320\n",
      "Epoch 165/200\n",
      "138/138 - 1s - loss: 0.0449 - MSE: 0.0449 - RMSE: 0.2118 - MAE: 0.1329 - val_loss: 0.0453 - val_MSE: 0.0453 - val_RMSE: 0.2128 - val_MAE: 0.1317\n",
      "Epoch 166/200\n",
      "138/138 - 1s - loss: 0.0442 - MSE: 0.0442 - RMSE: 0.2103 - MAE: 0.1320 - val_loss: 0.0442 - val_MSE: 0.0442 - val_RMSE: 0.2102 - val_MAE: 0.1308\n",
      "Epoch 167/200\n",
      "138/138 - 1s - loss: 0.0437 - MSE: 0.0437 - RMSE: 0.2090 - MAE: 0.1313 - val_loss: 0.0435 - val_MSE: 0.0435 - val_RMSE: 0.2085 - val_MAE: 0.1306\n",
      "Epoch 168/200\n",
      "138/138 - 1s - loss: 0.0433 - MSE: 0.0433 - RMSE: 0.2080 - MAE: 0.1306 - val_loss: 0.0435 - val_MSE: 0.0435 - val_RMSE: 0.2086 - val_MAE: 0.1303\n",
      "Epoch 169/200\n",
      "138/138 - 1s - loss: 0.0429 - MSE: 0.0429 - RMSE: 0.2072 - MAE: 0.1300 - val_loss: 0.0430 - val_MSE: 0.0430 - val_RMSE: 0.2074 - val_MAE: 0.1295\n",
      "Epoch 170/200\n",
      "138/138 - 1s - loss: 0.0428 - MSE: 0.0428 - RMSE: 0.2068 - MAE: 0.1298 - val_loss: 0.0425 - val_MSE: 0.0425 - val_RMSE: 0.2062 - val_MAE: 0.1286\n",
      "Epoch 171/200\n",
      "138/138 - 1s - loss: 0.0424 - MSE: 0.0424 - RMSE: 0.2058 - MAE: 0.1289 - val_loss: 0.0426 - val_MSE: 0.0426 - val_RMSE: 0.2064 - val_MAE: 0.1282\n",
      "Epoch 172/200\n",
      "138/138 - 1s - loss: 0.0422 - MSE: 0.0422 - RMSE: 0.2054 - MAE: 0.1284 - val_loss: 0.0425 - val_MSE: 0.0425 - val_RMSE: 0.2061 - val_MAE: 0.1278\n",
      "Epoch 173/200\n",
      "138/138 - 1s - loss: 0.0419 - MSE: 0.0419 - RMSE: 0.2047 - MAE: 0.1279 - val_loss: 0.0417 - val_MSE: 0.0417 - val_RMSE: 0.2043 - val_MAE: 0.1272\n",
      "Epoch 174/200\n",
      "138/138 - 1s - loss: 0.0414 - MSE: 0.0414 - RMSE: 0.2033 - MAE: 0.1271 - val_loss: 0.0426 - val_MSE: 0.0426 - val_RMSE: 0.2063 - val_MAE: 0.1286\n",
      "Epoch 175/200\n",
      "138/138 - 1s - loss: 0.0412 - MSE: 0.0412 - RMSE: 0.2031 - MAE: 0.1270 - val_loss: 0.0413 - val_MSE: 0.0413 - val_RMSE: 0.2033 - val_MAE: 0.1258\n",
      "Epoch 176/200\n",
      "138/138 - 1s - loss: 0.0410 - MSE: 0.0410 - RMSE: 0.2026 - MAE: 0.1262 - val_loss: 0.0410 - val_MSE: 0.0410 - val_RMSE: 0.2025 - val_MAE: 0.1255\n",
      "Epoch 177/200\n",
      "138/138 - 1s - loss: 0.0406 - MSE: 0.0406 - RMSE: 0.2014 - MAE: 0.1256 - val_loss: 0.0403 - val_MSE: 0.0403 - val_RMSE: 0.2007 - val_MAE: 0.1250\n",
      "Epoch 178/200\n",
      "138/138 - 1s - loss: 0.0402 - MSE: 0.0402 - RMSE: 0.2006 - MAE: 0.1249 - val_loss: 0.0399 - val_MSE: 0.0399 - val_RMSE: 0.1998 - val_MAE: 0.1242\n",
      "Epoch 179/200\n",
      "138/138 - 1s - loss: 0.0397 - MSE: 0.0397 - RMSE: 0.1993 - MAE: 0.1242 - val_loss: 0.0399 - val_MSE: 0.0399 - val_RMSE: 0.1997 - val_MAE: 0.1243\n",
      "Epoch 180/200\n",
      "138/138 - 1s - loss: 0.0395 - MSE: 0.0395 - RMSE: 0.1986 - MAE: 0.1238 - val_loss: 0.0393 - val_MSE: 0.0393 - val_RMSE: 0.1983 - val_MAE: 0.1231\n",
      "Epoch 181/200\n",
      "138/138 - 1s - loss: 0.0392 - MSE: 0.0392 - RMSE: 0.1979 - MAE: 0.1231 - val_loss: 0.0392 - val_MSE: 0.0392 - val_RMSE: 0.1980 - val_MAE: 0.1227\n",
      "Epoch 182/200\n",
      "138/138 - 1s - loss: 0.0388 - MSE: 0.0388 - RMSE: 0.1971 - MAE: 0.1223 - val_loss: 0.0392 - val_MSE: 0.0392 - val_RMSE: 0.1980 - val_MAE: 0.1233\n",
      "Epoch 183/200\n",
      "138/138 - 1s - loss: 0.0398 - MSE: 0.0398 - RMSE: 0.1995 - MAE: 0.1238 - val_loss: 0.0396 - val_MSE: 0.0396 - val_RMSE: 0.1991 - val_MAE: 0.1217\n",
      "Epoch 184/200\n",
      "138/138 - 1s - loss: 0.0388 - MSE: 0.0388 - RMSE: 0.1970 - MAE: 0.1220 - val_loss: 0.0390 - val_MSE: 0.0390 - val_RMSE: 0.1976 - val_MAE: 0.1219\n",
      "Epoch 185/200\n",
      "138/138 - 1s - loss: 0.0383 - MSE: 0.0383 - RMSE: 0.1958 - MAE: 0.1216 - val_loss: 0.0385 - val_MSE: 0.0385 - val_RMSE: 0.1963 - val_MAE: 0.1207\n",
      "Epoch 186/200\n",
      "138/138 - 1s - loss: 0.0381 - MSE: 0.0381 - RMSE: 0.1951 - MAE: 0.1210 - val_loss: 0.0386 - val_MSE: 0.0386 - val_RMSE: 0.1964 - val_MAE: 0.1204\n",
      "Epoch 187/200\n",
      "138/138 - 1s - loss: 0.0381 - MSE: 0.0381 - RMSE: 0.1952 - MAE: 0.1208 - val_loss: 0.0388 - val_MSE: 0.0388 - val_RMSE: 0.1971 - val_MAE: 0.1196\n",
      "Epoch 188/200\n",
      "138/138 - 1s - loss: 0.0383 - MSE: 0.0383 - RMSE: 0.1958 - MAE: 0.1210 - val_loss: 0.0388 - val_MSE: 0.0388 - val_RMSE: 0.1969 - val_MAE: 0.1196\n",
      "Epoch 189/200\n",
      "138/138 - 1s - loss: 0.0378 - MSE: 0.0378 - RMSE: 0.1944 - MAE: 0.1199 - val_loss: 0.0403 - val_MSE: 0.0403 - val_RMSE: 0.2006 - val_MAE: 0.1241\n",
      "Epoch 190/200\n",
      "138/138 - 1s - loss: 0.0378 - MSE: 0.0378 - RMSE: 0.1945 - MAE: 0.1202 - val_loss: 0.0380 - val_MSE: 0.0380 - val_RMSE: 0.1950 - val_MAE: 0.1189\n",
      "Epoch 191/200\n",
      "138/138 - 1s - loss: 0.0374 - MSE: 0.0374 - RMSE: 0.1933 - MAE: 0.1192 - val_loss: 0.0376 - val_MSE: 0.0376 - val_RMSE: 0.1940 - val_MAE: 0.1182\n",
      "Epoch 192/200\n",
      "138/138 - 1s - loss: 0.0369 - MSE: 0.0369 - RMSE: 0.1922 - MAE: 0.1187 - val_loss: 0.0368 - val_MSE: 0.0368 - val_RMSE: 0.1918 - val_MAE: 0.1179\n",
      "Epoch 193/200\n",
      "138/138 - 1s - loss: 0.0367 - MSE: 0.0367 - RMSE: 0.1915 - MAE: 0.1182 - val_loss: 0.0372 - val_MSE: 0.0372 - val_RMSE: 0.1927 - val_MAE: 0.1175\n",
      "Epoch 194/200\n",
      "138/138 - 1s - loss: 0.0365 - MSE: 0.0365 - RMSE: 0.1911 - MAE: 0.1180 - val_loss: 0.0366 - val_MSE: 0.0366 - val_RMSE: 0.1913 - val_MAE: 0.1170\n",
      "Epoch 195/200\n",
      "138/138 - 1s - loss: 0.0363 - MSE: 0.0363 - RMSE: 0.1904 - MAE: 0.1174 - val_loss: 0.0363 - val_MSE: 0.0363 - val_RMSE: 0.1904 - val_MAE: 0.1165\n",
      "Epoch 196/200\n",
      "138/138 - 1s - loss: 0.0359 - MSE: 0.0359 - RMSE: 0.1896 - MAE: 0.1169 - val_loss: 0.0360 - val_MSE: 0.0360 - val_RMSE: 0.1897 - val_MAE: 0.1161\n",
      "Epoch 197/200\n",
      "138/138 - 1s - loss: 0.0357 - MSE: 0.0357 - RMSE: 0.1891 - MAE: 0.1165 - val_loss: 0.0357 - val_MSE: 0.0357 - val_RMSE: 0.1890 - val_MAE: 0.1157\n",
      "Epoch 198/200\n",
      "138/138 - 1s - loss: 0.0355 - MSE: 0.0355 - RMSE: 0.1885 - MAE: 0.1161 - val_loss: 0.0354 - val_MSE: 0.0354 - val_RMSE: 0.1881 - val_MAE: 0.1154\n",
      "Epoch 199/200\n",
      "138/138 - 1s - loss: 0.0352 - MSE: 0.0352 - RMSE: 0.1877 - MAE: 0.1156 - val_loss: 0.0352 - val_MSE: 0.0352 - val_RMSE: 0.1876 - val_MAE: 0.1149\n",
      "Epoch 200/200\n",
      "138/138 - 1s - loss: 0.0351 - MSE: 0.0351 - RMSE: 0.1874 - MAE: 0.1152 - val_loss: 0.0356 - val_MSE: 0.0356 - val_RMSE: 0.1887 - val_MAE: 0.1160\n",
      "218.16532111167908 s to train model.\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Fit the network\n",
    "###############################################################################\n",
    "print ('\\nFitting the network.')\n",
    "startTime = time.time ()\n",
    "history = clf.fit (X_train, X_train,\n",
    "                  batch_size = BATCH_SIZE,\n",
    "                  epochs = NUMBER_OF_EPOCHS,\n",
    "                  verbose = 2, #1 = progress bar, not useful for logging\n",
    "                  workers = 0,\n",
    "                  use_multiprocessing = True,\n",
    "                  #class_weight = 'auto',\n",
    "                  validation_data = (X_val, X_val))\n",
    "print (str (time.time () - startTime), 's to train model.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error (MSE): 0.035373711953948644\n",
      "Validation error (MSE): 0.03562014814252987\n"
     ]
    }
   ],
   "source": [
    "###############################################################################\n",
    "## Analyze results\n",
    "###############################################################################\n",
    "X_val_pred   = clf.predict (X_val)\n",
    "X_train_pred = clf.predict (X_train)\n",
    "print ('Train error (MSE):'     , mean_squared_error (X_train_pred, X_train))\n",
    "print ('Validation error (MSE):', mean_squared_error (X_val_pred, X_val))\n",
    "\n",
    "#SAMPLES = 50\n",
    "#print ('Error on first', SAMPLES, 'samples:')\n",
    "#print ('MSE (pred, real)')\n",
    "#for pred_sample, real_sample in zip (X_val_pred [:SAMPLES], X_val [:SAMPLES]):\n",
    "#  print (mean_squared_error (pred_sample, real_sample))\n",
    "\n",
    "\n",
    "train_mse_element_wise = np.mean (np.square (X_train_pred - X_train), axis = 1)\n",
    "val_mse_element_wise = np.mean (np.square (X_val_pred - X_val), axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum validation error (MSE): 212.87403731227266\n",
      "Bottom 20 validation error (MSE): [0.0008386  0.00084068 0.00085071 0.00090994 0.0009103  0.00091041\n",
      " 0.00091067 0.00091085 0.00091107 0.00091107 0.00091124 0.00091149\n",
      " 0.00091162 0.00091175 0.00092287 0.0009243  0.00092491 0.00092505\n",
      " 0.00092514 0.00092527]\n",
      "Top 20 validation error (MSE): [212.87403731 152.36033234  88.81892743  68.15041191  49.10700713\n",
      "  41.67126179  40.90701572  38.22057451  37.83463887  34.11203636\n",
      "  33.02540706  30.37851064  29.06671766  28.99219376  26.58209769\n",
      "  24.12298189  21.20354268  20.444242    20.32900387  19.84391481]\n",
      "\n",
      "Number of samples considered: 4584\n",
      "Bottom 20 on considered samples (MSE): [0.2847333  0.28484252 0.28495044 0.28496539 0.28499881 0.28506731\n",
      " 0.28515559 0.28518685 0.28525053 0.28533156 0.28533821 0.28537382\n",
      " 0.28540863 0.28540896 0.285425   0.28547608 0.28557485 0.28559217\n",
      " 0.28565744 0.28565978]\n",
      "Top 20 on considered samples (MSE): [212.87403731 152.36033234  88.81892743  68.15041191  49.10700713\n",
      "  41.67126179  40.90701572  38.22057451  37.83463887  34.11203636\n",
      "  33.02540706  30.37851064  29.06671766  28.99219376  26.58209769\n",
      "  24.12298189  21.20354268  20.444242    20.32900387  19.84391481]\n",
      "Thresh val: 0.4945271230560512\n"
     ]
    }
   ],
   "source": [
    "max_threshold_val = np.max (val_mse_element_wise)\n",
    "val_mse_element_wise.sort ()\n",
    "print ('Maximum validation error (MSE):', max_threshold_val)\n",
    "print ('Bottom 20 validation error (MSE):', val_mse_element_wise [:20])\n",
    "print ('Top 20 validation error (MSE):', val_mse_element_wise [::-1][:20])\n",
    "\n",
    "\n",
    "### K: This looks like another hyperparameter to be adjusted by using a\n",
    "### separate validation set that contains normal and anomaly samples.\n",
    "### K: I've guessed 1%, this may be a future line of research.\n",
    "THRESHOLD_SAMPLE_PERCENTAGE = 1/100\n",
    "top_n_values_val = np.partition (-val_mse_element_wise,\n",
    "                                 int (round (val_mse_element_wise.shape [0] *\n",
    "                                             THRESHOLD_SAMPLE_PERCENTAGE)))\n",
    "\n",
    "top_n_values_val = -top_n_values_val [: int (round (val_mse_element_wise.shape [0] *\n",
    "                                                    THRESHOLD_SAMPLE_PERCENTAGE))]\n",
    "print ('\\nNumber of samples considered:', int (round (val_mse_element_wise.shape [0] *\n",
    "                                        THRESHOLD_SAMPLE_PERCENTAGE)))\n",
    "top_n_values_val.sort ()\n",
    "print ('Bottom 20 on considered samples (MSE):', top_n_values_val [:20])\n",
    "print ('Top 20 on considered samples (MSE):', top_n_values_val [::-1][:20])\n",
    "\n",
    "\n",
    "### K: O limiar de classificacao sera a mediana dos N maiores custos obtidos\n",
    "### ao validar a rede no conjunto de validacao. N e um hiperparametro que pode\n",
    "### ser ajustado, mas e necessario um conjunto de validacao com amostras\n",
    "### anomalas em adicao ao conjunto de validacao atual, que so tem amostras nao\n",
    "### anomalas. @TODO: Desenvolver e validar o conjunto com esta nova tecnica.\n",
    "threshold = np.median (top_n_values_val)\n",
    "print ('Thresh val:', threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(488, 15)\n",
      "Test error: 776801.8114204492\n",
      "\n",
      "Performance on TEST set:\n",
      "\n",
      "MSE (pred, real) | Label (ordered)\n",
      "Confusion matrix:\n",
      "tp | fp\n",
      "fn | tn\n",
      "244 | 6\n",
      "0 | 238\n",
      "TP: 244\n",
      "TN: 238\n",
      "FP: 6\n",
      "FN: 0\n"
     ]
    }
   ],
   "source": [
    "### K: NOTE: Only look at test results when publishing...\n",
    "#sys.exit ()\n",
    "X_test_pred = clf.predict (X_test)\n",
    "print (X_test_pred.shape)\n",
    "print ('Test error:', mean_squared_error (X_test_pred, X_test))\n",
    "\n",
    "\n",
    "y_pred = np.mean (np.square (X_test_pred - X_test), axis = 1)\n",
    "#y_pred = []\n",
    "#for pred_sample, real_sample, label in zip (X_test_pred, X_test, y_test):\n",
    "#  y_pred.append (mean_squared_error (pred_sample, real_sample))\n",
    "\n",
    "#print ('\\nLabel | MSE (pred, real)')\n",
    "#for label, pred in zip (y_test, y_pred):\n",
    "#  print (label, '|', pred)\n",
    "\n",
    "y_test, y_pred = zip (*sorted (zip (y_test, y_pred)))\n",
    "#print ('\\nLabel | MSE (pred, real) (ordered)')\n",
    "#for label, pred in zip (y_test, y_pred):\n",
    "#  print (label, '|', pred)\n",
    "\n",
    "# 0 == normal\n",
    "# 1 == attack\n",
    "print ('\\nPerformance on TEST set:')\n",
    "print ('\\nMSE (pred, real) | Label (ordered)')\n",
    "tp, tn, fp, fn = 0, 0, 0, 0\n",
    "for label, pred in zip (y_test, y_pred):\n",
    "#  if (pred >= threshold):\n",
    "#    print ('Classified as anomaly     (NORMAL):', label)\n",
    "#  else:\n",
    "#    print ('Classified as not anomaly (ATTACK):', label)\n",
    "\n",
    "  if ((pred >= threshold) and (label == 0)):\n",
    "#    print ('True negative.')\n",
    "    tn += 1\n",
    "  elif ((pred >= threshold) and (label == 1)):\n",
    "#    print ('False negative!')\n",
    "    fn += 1\n",
    "  elif ((pred < threshold) and (label == 1)):\n",
    "#    print ('True positive.')\n",
    "    tp += 1\n",
    "  elif ((pred < threshold) and (label == 0)):\n",
    "#    print ('False positive!')\n",
    "    fp += 1\n",
    "\n",
    "print ('Confusion matrix:')\n",
    "print ('tp | fp')\n",
    "print ('fn | tn')\n",
    "print (tp, '|', fp)\n",
    "print (fn, '|', tn)\n",
    "print ('TP:', tp)\n",
    "print ('TN:', tn)\n",
    "print ('FP:', fp)\n",
    "print ('FN:', fn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
