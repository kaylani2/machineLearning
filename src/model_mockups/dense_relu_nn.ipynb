{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "DEVICE = 'GPU/:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configuração para o data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.set_floatx('float64')\n",
    "\n",
    "#getting the csv\n",
    "CICIDS_DIRECTORY = '../../datasets/cicids/MachineLearningCVE/'\n",
    "CICIDS_MONDAY_FILENAME = 'Monday-WorkingHours.pcap_ISCX.csv'\n",
    "CICIDS_WEDNESDAY_FILENAME = 'Wednesday-workingHours.pcap_ISCX.csv'\n",
    "CICIDS_MONDAY = CICIDS_DIRECTORY + CICIDS_MONDAY_FILENAME\n",
    "CICIDS_WEDNESDAY = CICIDS_DIRECTORY + CICIDS_WEDNESDAY_FILENAME"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tratando o data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame = pd.read_csv(CICIDS_WEDNESDAY)\n",
    "## Remove NaN and inf values\n",
    "dataFrame.replace ('Infinity', np.nan, inplace = True) ## Or other text values\n",
    "dataFrame.replace (np.inf, np.nan, inplace = True) ## Remove infinity\n",
    "dataFrame.replace (np.nan, 0, inplace = True)\n",
    "\n",
    "newKeys = []\n",
    "for key in dataFrame.keys():\n",
    "    newKeys.append(key.replace(' ', '-'))\n",
    "\n",
    "dict_rename = { source:destination for source, destination in zip(dataFrame.keys(), newKeys)}\n",
    "dataFrame = dataFrame.rename(columns=dict_rename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting labels\n",
    "dataFrame ['-Label'] = dataFrame ['-Label'].replace ('BENIGN', 0)\n",
    "dataFrame ['-Label'] = dataFrame ['-Label'].replace ('DoS slowloris', 1)\n",
    "dataFrame ['-Label'] = dataFrame ['-Label'].replace ('DoS Slowhttptest', 2)\n",
    "dataFrame ['-Label'] = dataFrame ['-Label'].replace ('DoS Hulk', 3)\n",
    "dataFrame ['-Label'] = dataFrame ['-Label'].replace ('DoS GoldenEye', 4)\n",
    "dataFrame ['-Label'] = dataFrame ['-Label'].replace ('Heartbleed', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "443329 train examples\n",
      "110833 validation examples\n",
      "138541 test examples\n"
     ]
    }
   ],
   "source": [
    "#splitting dataset\n",
    "train, test = train_test_split(dataFrame, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.2)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make dataFrame into a data set\n",
    "def df_to_dataset(dataFrame, shuffle=True, batch_size=32):\n",
    "    dataFrame = dataFrame.copy()\n",
    "    labels = dataFrame.pop('-Label')\n",
    "    data_set = tf.data.Dataset.from_tensor_slices((dict(dataFrame), labels))\n",
    "    if shuffle:\n",
    "        data_set = data_set.shuffle(buffer_size=len(dataFrame))\n",
    "    data_set = data_set.batch(batch_size)\n",
    "    return data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#transform each part of the dataFrame into the data_set format\n",
    "BATCH_SIZE = 32\n",
    "train_ds = df_to_dataset(train, batch_size=BATCH_SIZE)\n",
    "val_ds = df_to_dataset(val, shuffle=False, batch_size=BATCH_SIZE)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature columns preset for the model\n",
    "Note: All columns of this dataset are numeric\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st approach: using all columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "numeric_headers = []\n",
    "categorical_headers = []\n",
    "count = 0\n",
    "for feature, label in train_ds.take(1):\n",
    "    for key in list(feature.keys()):\n",
    "        feature_columns.append(feature_column.numeric_column(key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "initializer = tf.initializers.VarianceScaling(scale=2.0)\n",
    "hidden_layer_size, num_classes = 128, 6\n",
    "\n",
    "layers = [\n",
    "    feature_layer,\n",
    "    tf.keras.layers.Dense(hidden_layer_size, use_bias=True, activation='relu', kernel_initializer=initializer),\n",
    "    tf.keras.layers.Dense(hidden_layer_size,  use_bias=True, activation='relu', kernel_initializer=initializer),\n",
    "    tf.keras.layers.Dense(num_classes,  use_bias=True, activation='softmax', kernel_initializer=initializer),\n",
    "]\n",
    "\n",
    "model = tf.keras.Sequential(layers)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=[tf.keras.metrics.sparse_categorical_accuracy])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "WARNING:tensorflow:Layer dense is casting an input tensor from dtype float32 to the layer's dtype of float64, which is new behavior in TensorFlow 2.  The layer has dtype float64 because it's dtype defaults to floatx.\n",
      "\n",
      "If you intended to run this layer in float64, you can safely ignore this warning. If in doubt, this warning is likely only an issue if you are porting a TensorFlow 1.X model to TensorFlow 2.\n",
      "\n",
      "To change all layers to have dtype float32 by default, call `tf.keras.backend.set_floatx('float32')`. To change just this layer, pass dtype='float32' to the layer constructor. If you are the author of this layer, you can disable autocasting by passing autocast=False to the base Layer constructor.\n",
      "\n",
      "13855/13855 [==============================] - 63s 5ms/step - loss: 49182.3925 - sparse_categorical_accuracy: 0.8962 - val_loss: 277.5948 - val_sparse_categorical_accuracy: 0.7800\n",
      "Epoch 2/5\n",
      " 5792/13855 [===========>..................] - ETA: 37s - loss: 88.3112 - sparse_categorical_accuracy: 0.7452"
     ]
    }
   ],
   "source": [
    "with tf.device(DEVICE):\n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
